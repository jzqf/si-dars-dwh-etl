<transformation>
  <info>
    <name>tr_create_job_summary_file_with_injection</name>
    <description />
    <extended_description />
    <trans_version />
    <trans_type>Normal</trans_type>
    <trans_status>0</trans_status>
    <directory>/generic/create_job_summary_file</directory>
    <parameters>
      <parameter>
        <name>PARAM_REPO_PATH_TO_JOB_SUMMARY_SQL</name>
        <default_value />
        <description>Path within the PDI repository to the SQL script to run to generate the SQL summary</description>
      </parameter>
    </parameters>
    <log>
      <trans-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANS_SCHEMA}</schema>
        <table>${QF_LOG_TRANS_TABLE}</table>
        <size_limit_lines />
        <interval />
        <timeout_days>${QF_LOG_TRANS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STATUS</id>
          <enabled>Y</enabled>
          <name>STATUS</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
          <subject />
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
          <subject />
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
          <subject />
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
          <subject />
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
          <subject />
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
          <subject />
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>STARTDATE</id>
          <enabled>Y</enabled>
          <name>STARTDATE</name>
        </field>
        <field>
          <id>ENDDATE</id>
          <enabled>Y</enabled>
          <name>ENDDATE</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>DEPDATE</id>
          <enabled>Y</enabled>
          <name>DEPDATE</name>
        </field>
        <field>
          <id>REPLAYDATE</id>
          <enabled>Y</enabled>
          <name>REPLAYDATE</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>Y</enabled>
          <name>LOG_FIELD</name>
        </field>
        <field>
          <id>EXECUTING_SERVER</id>
          <enabled>N</enabled>
          <name>EXECUTING_SERVER</name>
        </field>
        <field>
          <id>EXECUTING_USER</id>
          <enabled>N</enabled>
          <name>EXECUTING_USER</name>
        </field>
        <field>
          <id>CLIENT</id>
          <enabled>N</enabled>
          <name>CLIENT</name>
        </field>
      </trans-log-table>
      <perf-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANSPERF_SCHEMA}</schema>
        <table>${QF_LOG_TRANSPERF_TABLE}</table>
        <interval />
        <timeout_days>${QF_LOG_TRANSPERF_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>SEQ_NR</id>
          <enabled>Y</enabled>
          <name>SEQ_NR</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>INPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>INPUT_BUFFER_ROWS</name>
        </field>
        <field>
          <id>OUTPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>OUTPUT_BUFFER_ROWS</name>
        </field>
      </perf-log-table>
      <channel-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_CHANNEL_SCHEMA}</schema>
        <table>${QF_LOG_CHANNEL_TABLE}</table>
        <timeout_days>${QF_LOG_CHANNEL_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>LOGGING_OBJECT_TYPE</id>
          <enabled>Y</enabled>
          <name>LOGGING_OBJECT_TYPE</name>
        </field>
        <field>
          <id>OBJECT_NAME</id>
          <enabled>Y</enabled>
          <name>OBJECT_NAME</name>
        </field>
        <field>
          <id>OBJECT_COPY</id>
          <enabled>Y</enabled>
          <name>OBJECT_COPY</name>
        </field>
        <field>
          <id>REPOSITORY_DIRECTORY</id>
          <enabled>Y</enabled>
          <name>REPOSITORY_DIRECTORY</name>
        </field>
        <field>
          <id>FILENAME</id>
          <enabled>Y</enabled>
          <name>FILENAME</name>
        </field>
        <field>
          <id>OBJECT_ID</id>
          <enabled>Y</enabled>
          <name>OBJECT_ID</name>
        </field>
        <field>
          <id>OBJECT_REVISION</id>
          <enabled>Y</enabled>
          <name>OBJECT_REVISION</name>
        </field>
        <field>
          <id>PARENT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>PARENT_CHANNEL_ID</name>
        </field>
        <field>
          <id>ROOT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>ROOT_CHANNEL_ID</name>
        </field>
      </channel-log-table>
      <step-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANSSTEP_SCHEMA}</schema>
        <table>${QF_LOG_TRANSSTEP_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSSTEP_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>N</enabled>
          <name>LOG_FIELD</name>
        </field>
      </step-log-table>
      <metrics-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANSMETRICS_SCHEMA}</schema>
        <table>${QF_LOG_TRANSMETRICS_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSMETRICS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>METRICS_DATE</id>
          <enabled>Y</enabled>
          <name>METRICS_DATE</name>
        </field>
        <field>
          <id>METRICS_CODE</id>
          <enabled>Y</enabled>
          <name>METRICS_CODE</name>
        </field>
        <field>
          <id>METRICS_DESCRIPTION</id>
          <enabled>Y</enabled>
          <name>METRICS_DESCRIPTION</name>
        </field>
        <field>
          <id>METRICS_SUBJECT</id>
          <enabled>Y</enabled>
          <name>METRICS_SUBJECT</name>
        </field>
        <field>
          <id>METRICS_TYPE</id>
          <enabled>Y</enabled>
          <name>METRICS_TYPE</name>
        </field>
        <field>
          <id>METRICS_VALUE</id>
          <enabled>Y</enabled>
          <name>METRICS_VALUE</name>
        </field>
      </metrics-log-table>
    </log>
    <maxdate>
      <connection />
      <table />
      <field />
      <offset>0.0</offset>
      <maxdiff>0.0</maxdiff>
    </maxdate>
    <size_rowset>10000</size_rowset>
    <sleep_time_empty>50</sleep_time_empty>
    <sleep_time_full>50</sleep_time_full>
    <unique_connections>N</unique_connections>
    <feedback_shown>Y</feedback_shown>
    <feedback_size>50000</feedback_size>
    <using_thread_priorities>Y</using_thread_priorities>
    <shared_objects_file />
    <capture_step_performance>Y</capture_step_performance>
    <step_performance_capturing_delay>1000</step_performance_capturing_delay>
    <step_performance_capturing_size_limit>100</step_performance_capturing_size_limit>
    <dependencies>
    </dependencies>
    <partitionschemas>
    </partitionschemas>
    <slaveservers>
    </slaveservers>
    <clusterschemas>
    </clusterschemas>
    <created_user>-</created_user>
    <created_date>2016/04/03 09:57:16.279</created_date>
    <modified_user>-</modified_user>
    <modified_date>2018/11/14 15:24:38.595</modified_date>
    <key_for_session_key>H4sIAAAAAAAAAAMAAAAAAAAAAAA=</key_for_session_key>
    <is_key_private>N</is_key_private>
  </info>
  <notepads>
    <notepad>
      <note>This transformation creates a "job summary" file that can, e.g., be attached to an email. This job has parameters:

	PARAM_REPO_PATH_TO_JOB_SUMMARY_SQL		Path within the PDI repository to the SQL script to run to generate the SQL summary</note>
      <xloc>0</xloc>
      <yloc>0</yloc>
      <width>787</width>
      <heigth>49</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>If the query loaded from the SQL file is blank/empty, a default query is specifed here.
This is not so much a convenience, but rather to avoid a runtime error with the ETL
metadata injection of the query into the template transformation below. If the
query that is injected is blank, a runtime error is thrown.

Note:	It was necessary to cast the string returned from this query as VARCHAR(128).
		If this is not done, it seems that the presence of the SQL concatenation operator,
		"||" in the query, causes PDI to treat the resulting string as an arbitrary length
		 text value. This, in turn, triggers an error in the "Text file output" step of the
		template transformation because of some size limitation. *Why* this occurs is
		not clear, but the error reported by PDI is:

			java.lang.OutOfMemoryError: Requested array size exceeds VM limit

		Simply casting the string as VARCHAR(128) avoids this error.</note>
      <xloc>288</xloc>
      <yloc>128</yloc>
      <width>503</width>
      <heigth>205</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>This transformation does not work. See the notes in the template transformation, 
"tmplt_create_job_summary_file", for details.</note>
      <xloc>800</xloc>
      <yloc>0</yloc>
      <width>461</width>
      <heigth>36</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>255</backgroundcolorgreen>
      <backgroundcolorblue>0</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <connection>
    <name>logging_db</name>
    <server>${QF_LOGGING_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_LOGGING_DB_DATABASE}</database>
    <port>${QF_LOGGING_DB_PORT}</port>
    <username>${QF_LOGGING_DB_USERNAME}</username>
    <password>${QF_LOGGING_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_LOGGING_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <order>
    <hop>
      <from>Log "ETL job summary" query</from>
      <to>tmplt_create_job_summary_file</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Load SQL query in memory</from>
      <to>Specify default query if one not specified</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Specify default query if one not specified</from>
      <to>Log "ETL job summary" query</to>
      <enabled>Y</enabled>
    </hop>
  </order>
  <step>
    <name>Load SQL query in memory</name>
    <type>LoadFileInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <include>N</include>
    <include_field />
    <rownum>N</rownum>
    <addresultfile>N</addresultfile>
    <IsIgnoreEmptyFile>N</IsIgnoreEmptyFile>
    <IsIgnoreMissingPath>N</IsIgnoreMissingPath>
    <rownum_field />
    <encoding>UTF-8</encoding>
    <file>
      <name>${KETTLE_HOME}/${QF_PATH_TO_REPO_FROM_KETTLE_HOME}/${PARAM_REPO_PATH_TO_JOB_SUMMARY_SQL}</name>
      <filemask />
      <exclude_filemask />
      <file_required>N</file_required>
      <include_subfolders>N</include_subfolders>
    </file>
    <fields>
      <field>
        <name>sql_query</name>
        <element_type>content</element_type>
        <type>String</type>
        <format />
        <currency />
        <decimal />
        <group />
        <length>-1</length>
        <precision>-1</precision>
        <trim_type>none</trim_type>
        <repeat>N</repeat>
      </field>
    </fields>
    <limit>0</limit>
    <IsInFields>N</IsInFields>
    <DynamicFilenameField />
    <shortFileFieldName />
    <pathFieldName />
    <hiddenFieldName />
    <lastModificationTimeFieldName />
    <uriNameFieldName />
    <rootUriNameFieldName />
    <extensionFieldName />
    <attributes />
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>144</xloc>
      <yloc>80</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Log "ETL job summary" query</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_minimal</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>PARAM_REPO_PATH_TO_JOB_SUMMARY_SQL = ${PARAM_REPO_PATH_TO_JOB_SUMMARY_SQL}</logmessage>
    <fields>
      <field>
        <name>sql_query</name>
      </field>
    </fields>
    <attributes />
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>144</xloc>
      <yloc>352</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Specify default query if one not specified</name>
    <type>ValueMapper</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <field_to_use>sql_query</field_to_use>
    <target_field />
    <non_match_default />
    <fields>
      <field>
        <source_value />
        <target_value>SELECT CAST('Job finished at ' || current_timestamp AS VARCHAR(128)) AS "job_summary";</target_value>
      </field>
    </fields>
    <attributes />
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>144</xloc>
      <yloc>208</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>tmplt_create_job_summary_file</name>
    <type>MetaInject</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <specification_method>rep_name</specification_method>
    <trans_object_id />
    <trans_name>tmplt_create_job_summary_file</trans_name>
    <filename />
    <directory_path>${Internal.Entry.Current.Directory}</directory_path>
    <source_step />
    <source_output_fields>    </source_output_fields>
    <target_file />
    <no_execution>N</no_execution>
    <stream_source_step />
    <stream_target_step />
    <mappings>
      <mapping>
        <target_step_name>Save stream from "job summary" query as CSV file</target_step_name>
        <target_attribute_key>ADD_TO_RESULT</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step />
        <source_field>Y</source_field>
      </mapping>
      <mapping>
        <target_step_name>Save stream from "job summary" query as CSV file</target_step_name>
        <target_attribute_key>SEPARATOR</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step />
        <source_field>;</source_field>
      </mapping>
      <mapping>
        <target_step_name>Save stream from "job summary" query as CSV file</target_step_name>
        <target_attribute_key>CREATE_PARENT_FOLDER</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step />
        <source_field>N</source_field>
      </mapping>
      <mapping>
        <target_step_name>Run "job summary" SQL squery</target_step_name>
        <target_attribute_key>SQL</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step>Log "ETL job summary" query</source_step>
        <source_field>sql_query</source_field>
      </mapping>
      <mapping>
        <target_step_name>Save stream from "job summary" query as CSV file</target_step_name>
        <target_attribute_key>HEADER</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step />
        <source_field>Y</source_field>
      </mapping>
      <mapping>
        <target_step_name>Save stream from "job summary" query as CSV file</target_step_name>
        <target_attribute_key>EXTENSION</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step />
        <source_field>csv</source_field>
      </mapping>
      <mapping>
        <target_step_name>Save stream from "job summary" query as CSV file</target_step_name>
        <target_attribute_key>FILENAME</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step />
        <source_field>/tmp/job_summary</source_field>
      </mapping>
      <mapping>
        <target_step_name>Save stream from "job summary" query as CSV file</target_step_name>
        <target_attribute_key>ENCLOSURE</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step />
        <source_field>"</source_field>
      </mapping>
    </mappings>
    <attributes />
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>144</xloc>
      <yloc>432</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step_error_handling>
  </step_error_handling>
  <slave-step-copy-partition-distribution>
  </slave-step-copy-partition-distribution>
  <slave_transformation>N</slave_transformation>
  <attributes />
</transformation>
