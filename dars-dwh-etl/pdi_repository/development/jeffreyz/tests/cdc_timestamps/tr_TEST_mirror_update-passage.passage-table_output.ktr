<transformation>
  <info>
    <name>tr_TEST_mirror_update-passage.passage-table_output</name>
    <description />
    <extended_description />
    <trans_version />
    <trans_type>Normal</trans_type>
    <trans_status>0</trans_status>
    <directory>/development/jeffreyz/tests/cdc_timestamps</directory>
    <parameters>
      <parameter>
        <name>PARAM_CDC_CURRENT_LOAD_UTC</name>
        <default_value>1900.01.01 00:00:00</default_value>
        <description>Timestamp of the current load (no rows after this timestamp will be loaded)</description>
      </parameter>
      <parameter>
        <name>PARAM_CDC_LAST_LOAD_UTC</name>
        <default_value>1900.01.01 00:00:00</default_value>
        <description>Timestamp of last successful load from the source database</description>
      </parameter>
    </parameters>
    <log>
      <trans-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANS_SCHEMA}</schema>
        <table>${QF_LOG_TRANS_TABLE}</table>
        <size_limit_lines />
        <interval />
        <timeout_days>${QF_LOG_TRANS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STATUS</id>
          <enabled>Y</enabled>
          <name>STATUS</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
          <subject>Table output - DWH.passage.passage 2</subject>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
          <subject>Table output - DWH.passage.passage 2</subject>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
          <subject>Table output - DWH.passage.passage 2</subject>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
          <subject>Table input -  TDP.passage.passage</subject>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
          <subject>Table output - DWH.passage.passage 2</subject>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
          <subject>Table output - DWH.passage.passage 2</subject>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>STARTDATE</id>
          <enabled>Y</enabled>
          <name>STARTDATE</name>
        </field>
        <field>
          <id>ENDDATE</id>
          <enabled>Y</enabled>
          <name>ENDDATE</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>DEPDATE</id>
          <enabled>Y</enabled>
          <name>DEPDATE</name>
        </field>
        <field>
          <id>REPLAYDATE</id>
          <enabled>Y</enabled>
          <name>REPLAYDATE</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>Y</enabled>
          <name>LOG_FIELD</name>
        </field>
        <field>
          <id>EXECUTING_SERVER</id>
          <enabled>N</enabled>
          <name>EXECUTING_SERVER</name>
        </field>
        <field>
          <id>EXECUTING_USER</id>
          <enabled>N</enabled>
          <name>EXECUTING_USER</name>
        </field>
        <field>
          <id>CLIENT</id>
          <enabled>N</enabled>
          <name>CLIENT</name>
        </field>
      </trans-log-table>
      <perf-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANSPERFORMANCE_SCHEMA}</schema>
        <table>${QF_LOG_TRANSPERFORMANCE_TABLE}</table>
        <interval />
        <timeout_days>${QF_LOG_TRANSPERFORMANCE_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>SEQ_NR</id>
          <enabled>Y</enabled>
          <name>SEQ_NR</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>INPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>INPUT_BUFFER_ROWS</name>
        </field>
        <field>
          <id>OUTPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>OUTPUT_BUFFER_ROWS</name>
        </field>
      </perf-log-table>
      <channel-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_CHANNEL_SCHEMA}</schema>
        <table>${QF_LOG_CHANNEL_TABLE}</table>
        <timeout_days>${QF_LOG_CHANNEL_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>LOGGING_OBJECT_TYPE</id>
          <enabled>Y</enabled>
          <name>LOGGING_OBJECT_TYPE</name>
        </field>
        <field>
          <id>OBJECT_NAME</id>
          <enabled>Y</enabled>
          <name>OBJECT_NAME</name>
        </field>
        <field>
          <id>OBJECT_COPY</id>
          <enabled>Y</enabled>
          <name>OBJECT_COPY</name>
        </field>
        <field>
          <id>REPOSITORY_DIRECTORY</id>
          <enabled>Y</enabled>
          <name>REPOSITORY_DIRECTORY</name>
        </field>
        <field>
          <id>FILENAME</id>
          <enabled>Y</enabled>
          <name>FILENAME</name>
        </field>
        <field>
          <id>OBJECT_ID</id>
          <enabled>Y</enabled>
          <name>OBJECT_ID</name>
        </field>
        <field>
          <id>OBJECT_REVISION</id>
          <enabled>Y</enabled>
          <name>OBJECT_REVISION</name>
        </field>
        <field>
          <id>PARENT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>PARENT_CHANNEL_ID</name>
        </field>
        <field>
          <id>ROOT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>ROOT_CHANNEL_ID</name>
        </field>
      </channel-log-table>
      <step-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANSSTEP_SCHEMA}</schema>
        <table>${QF_LOG_TRANSSTEP_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSSTEP_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>N</enabled>
          <name>LOG_FIELD</name>
        </field>
      </step-log-table>
      <metrics-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANSMETRICS_SCHEMA}</schema>
        <table>${QF_LOG_TRANSMETRICS_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSMETRICS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>METRICS_DATE</id>
          <enabled>Y</enabled>
          <name>METRICS_DATE</name>
        </field>
        <field>
          <id>METRICS_CODE</id>
          <enabled>Y</enabled>
          <name>METRICS_CODE</name>
        </field>
        <field>
          <id>METRICS_DESCRIPTION</id>
          <enabled>Y</enabled>
          <name>METRICS_DESCRIPTION</name>
        </field>
        <field>
          <id>METRICS_SUBJECT</id>
          <enabled>Y</enabled>
          <name>METRICS_SUBJECT</name>
        </field>
        <field>
          <id>METRICS_TYPE</id>
          <enabled>Y</enabled>
          <name>METRICS_TYPE</name>
        </field>
        <field>
          <id>METRICS_VALUE</id>
          <enabled>Y</enabled>
          <name>METRICS_VALUE</name>
        </field>
      </metrics-log-table>
    </log>
    <maxdate>
      <connection />
      <table />
      <field />
      <offset>0.0</offset>
      <maxdiff>0.0</maxdiff>
    </maxdate>
    <size_rowset>10000</size_rowset>
    <sleep_time_empty>50</sleep_time_empty>
    <sleep_time_full>50</sleep_time_full>
    <unique_connections>N</unique_connections>
    <feedback_shown>Y</feedback_shown>
    <feedback_size>50000</feedback_size>
    <using_thread_priorities>Y</using_thread_priorities>
    <shared_objects_file />
    <capture_step_performance>Y</capture_step_performance>
    <step_performance_capturing_delay>1000</step_performance_capturing_delay>
    <step_performance_capturing_size_limit>100</step_performance_capturing_size_limit>
    <dependencies>
    </dependencies>
    <partitionschemas>
    </partitionschemas>
    <slaveservers>
    </slaveservers>
    <clusterschemas>
    </clusterschemas>
    <created_user>-</created_user>
    <created_date>2016/04/03 09:57:16.279</created_date>
    <modified_user>-</modified_user>
    <modified_date>2017/06/23 16:10:56.635</modified_date>
    <key_for_session_key>H4sIAAAAAAAAAAMAAAAAAAAAAAA=</key_for_session_key>
    <is_key_private>N</is_key_private>
  </info>
  <notepads>
    <notepad>
      <note>Load the data from the source database table that will be moved to the DWH table.

It is important here to select "replace variables in script?"so that the references "?",
${PARAM_CDC_LAST_LOAD_UTC} and ${PARAM_CDC_CURRENT_LOAD_UTC} will be evaluated.

It is also necessary to specify the step from which fields will  be used to supply values 
for the "?" replacement characters.  In this case, the previous step is specified.</note>
      <xloc>288</xloc>
      <yloc>288</yloc>
      <width>512</width>
      <heigth>108</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Insert new rows into the DWH using a "Table output" step. Updates are handled by the
"Update" step via an error handling hop. This requires "passage_id" to have a unique
constraint; otherwise, error handling will not be triggered when attempted to insert a
new row with a duplicate key.

PDI does not fully support error handling when using "batch processing" with the 
PostgreSQL JDBC driver (a message to this effect is displayed when accepting the
transformation settings dialog), so make sure "Use batch update for inserts" is *not* 
selected for the "Table output" step!

But this approach is definitely faster than using only an Insert/Update step.</note>
      <xloc>848</xloc>
      <yloc>464</yloc>
      <width>513</width>
      <heigth>164</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Get batch ID for current transformation. This will be stored in the DWH table.</note>
      <xloc>288</xloc>
      <yloc>432</yloc>
      <width>459</width>
      <heigth>24</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>This transformation updates the DWH mirror of the "dynamic" table passage.passage from the TDP archive database.
This transformation has parameters:

	PARAM_CDC_LAST_LOAD_UTC		&lt;- Not used in this transformation - MAX(archive_id) is used instead
	PARAM_CDC_CURRENT_LOAD_UTC</note>
      <xloc>0</xloc>
      <yloc>0</yloc>
      <width>695</width>
      <heigth>80</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>batch=100, all inserts:		700 -> 140 rows/s
batch=1000, all inserts:		700 -> 145 rows/s
batch=100, all updates:		80 rows/s
batch=1000, all updates:	80 rows/s

After adding PRIMARY KEY constraint on 
"passage_id":

batch=100, all inserts:		1400 rows/s
batch=100, all updates: 		3200 rows/s

batch=1000, all inserts:		1600 rows/s
batch=1000, all updates:	3300 rows/s	</note>
      <xloc>304</xloc>
      <yloc>576</yloc>
      <width>280</width>
      <heigth>192</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>With error handling enabled for the "Table output" step, with or without 
error handling hop and using *either* an "Update" step or a "Dummy" 
step for error handling:

All inserts:		Update:batch=100,		Table output:batch=1000:	3200 rows/s
All updates:	Update:batch=100,		Table output:batch=1000:	2500 rows/s

All inserts:		Update:batch=100,		Table output:batch=1000:	3200 rows/s
All updates:	Update:batch=100,		Table output:batch=1000:	2500 rows/s

All inserts:		Update:batch=1000,	Table output:batch=1000:	3200 rows/s
All updates:	Update:batch=1000,	Table output:batch=1000:	2500 rows/s

All inserts:		Update:batch=100,		Table output:batch=0:		3200 rows/s
All updates:	Update:batch=100,		Table output:batch=0:		2500 rows/s

All inserts:		Update:batch=0,		Table output:batch=0:		3200 rows/s
All updates:	Update:batch=0,		Table output:batch=0:		2500 rows/s

This testing shows that we still get approximately the same throughput 
regardless of "commit size" and whether or not  "batch updates" are enabled 
in either the Table output or the Update step. THIS *MAY* MEAN THAT PDI 
DOES NOT ACTUALLY USE BATCH UPDATES AT ALL FOR POSTGRESQL IF 
ERROR HANDLING IS ENABLED, EVEN IF THE SETTINGS SPECIFY THAT BATCH 
UPDATES SHOULD BE USED. Yes, this seems to be the case. See:

http://forums.pentaho.com/showthread.php?71507-Batch-insert-mode-disabled-because-of-database-limitations-on-Postgres</note>
      <xloc>848</xloc>
      <yloc>640</yloc>
      <width>747</width>
      <heigth>388</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>This step handles only inserts, so it is not 
equivalent to the other approaches.For
this reason, its benchmark values are
higher.

batch=0		(inserts only):	4500 rows/s
batch=100		(inserts only):	5700 rows/s
batch=1000		(inserts only):	5800 rows/s
batch=5000		(inserts only):	6100 rows/s</note>
      <xloc>0</xloc>
      <yloc>576</yloc>
      <width>273</width>
      <heigth>136</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>An odd behaviour is observed using *either" the "Insert/Update" step or the 
"Table output" step with error handling (although in this second approach it
only happens when the error handling sends rows to the "Update" step):

After processing a large number of rows, it takes a long time for this 
transformation to end. During this period the "Pause" and "Stop" buttons  
remain enabled. Furthermore, running" top" shows that both "postgres"and 
"java" processes are actively using a 25-75% of the CPU. When this CPU usage 
stops, the  "Pause" and "Stop" buttons become disabled, showing that the
transformation is finally finished. Strange.

This does *not* occur when the "Update" step is replaced with a Dummy step,
which currently is acceptable if we do not need to perform updates -  i.e., if we
only need to be able to handle re-running an ETL script that tries to re-process
a row that was processed earlier during a load that failed part-way through.</note>
      <xloc>304</xloc>
      <yloc>768</yloc>
      <width>466</width>
      <heigth>220</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>255</backgroundcolorgreen>
      <backgroundcolorblue>0</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Using a "Table output" step with an error handling hop to an "Update" step 
provides an approximately 2x speedup over an "Insert/Update" step for 
inserts *and* it does not exhibit (for inserts only) the behaviour where a 
"java"and "postgres" process continue to run with a high CPU load long after 
the transformation is finished.</note>
      <xloc>320</xloc>
      <yloc>1216</yloc>
      <width>534</width>
      <heigth>95</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>173</backgroundcolorred>
      <backgroundcolorgreen>216</backgroundcolorgreen>
      <backgroundcolorblue>230</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Write parameters and max_target_insert_id value to the log so it will be obvious which values 
were used.</note>
      <xloc>288</xloc>
      <yloc>224</yloc>
      <width>479</width>
      <heigth>38</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Get the maximum value stored in column "archive_id" of the DWH table that will 
be updated by this transformation. This will be used to the select data from the 
source table in the TDP archive database that was inserted since the last update. 

IMPORTANT:	We *cannot* use MAX(archive_date) for this purpose because the
				"archive_date" column will not, in general, be unique.</note>
      <xloc>288</xloc>
      <yloc>96</yloc>
      <width>487</width>
      <heigth>94</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>If we use ${PARAM_CDC_LAST_LOAD_UTC} instead of MAX(archive_date) in the "Table input" 
query for loading rows from the TDP archive DB, then we need to replace the "Table output"
step without error handling that is currently used with *either* the "Insert/Update" step 
*or* the "Table output" step _with_ error handling.

This is because we will need to be able to handle the processing of rows from the TDP
archive DB that have already been loaded into the DWH. This will occur if a load fails for
some reason after successfully loading some records. In this case, the column
cdc_timestamps.last_successful_load will not be updated; hence, the next load will use
the same value for cdc_timestamps.last_successful_load, which will load rows that were
successfully processed during the previous load.

If we use the "Table output" step with error handling, then it is ESSENTIAL that there is a
unique constraint on the column "passage_id". This can be done by explicitly adding such
a constraint, or by making a primary key based on this column.</note>
      <xloc>800</xloc>
      <yloc>0</yloc>
      <width>547</width>
      <heigth>220</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>255</backgroundcolorgreen>
      <backgroundcolorblue>0</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>&lt;- More info:	If we were to re-run a load because of an error during a previous load, 
				we would likely either reload some of the same rows again that have 
				the same value of MAX(archive_date), or we would miss loading some 
				rows that have the same value of MAX(archive_date) (they would never
				 be loaded), depending on the comparator used with this value.</note>
      <xloc>816</xloc>
      <yloc>240</yloc>
      <width>522</width>
      <heigth>80</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>0</backgroundcolorgreen>
      <backgroundcolorblue>0</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <connection>
    <name>dwh_db</name>
    <server>${QF_DWH_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_DWH_DB_DATABASE}</database>
    <port>${QF_DWH_DB_PORT}</port>
    <username>${QF_DWH_DB_USERNAME}</username>
    <password>${QF_DWH_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_DWH_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <connection>
    <name>logging_db</name>
    <server>${QF_LOGGING_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_LOGGING_DB_DATABASE}</database>
    <port>${QF_LOGGING_DB_PORT}</port>
    <username>${QF_LOGGING_DB_USERNAME}</username>
    <password>${QF_LOGGING_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_LOGGING_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <connection>
    <name>obo_opr_db</name>
    <server>${QF_OBO_OPR_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_OBO_OPR_DB_DATABASE}</database>
    <port>${QF_OBO_OPR_DB_PORT}</port>
    <username>${QF_OBO_OPR_DB_USERNAME}</username>
    <password>${QF_OBO_OPR_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_OBO_OPR_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <order>
    <hop>
      <from>Table input -  TDP.passage.passage</from>
      <to>Get System Info - transformation batch ID</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Get System Info - transformation batch ID</from>
      <to>Table output - DWH.passage.passage 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Get System Info - transformation batch ID</from>
      <to>Insert / Update - DWH.passage.passage</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Get System Info - transformation batch ID</from>
      <to>Table output - DWH.passage.passage</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Table output - DWH.passage.passage 2</from>
      <to>Update - DWH.passage.passage</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Table output - DWH.passage.passage 2</from>
      <to>Dummy</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Get max ID for CDC: DWH.passage.passage</from>
      <to>Write to log: parameters, max_target_insert_id</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Write to log: parameters, max_target_insert_id</from>
      <to>Table input -  TDP.passage.passage</to>
      <enabled>Y</enabled>
    </hop>
  </order>
  <step>
    <name>Dummy</name>
    <type>Dummy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>784</xloc>
      <yloc>672</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get System Info - transformation batch ID</name>
    <type>SystemInfo</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <name>etl_batch_id_insert</name>
        <type>batch ID</type>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>112</xloc>
      <yloc>432</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get max ID for CDC: DWH.passage.passage</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dwh_db</connection>
    <sql>SELECT
    COALESCE(MAX(archive_id), ${QF_INSERT_ID_LOWER_BOUND}) AS max_target_insert_id
FROM
    passage.passage
</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>112</xloc>
      <yloc>144</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Insert / Update - DWH.passage.passage</name>
    <type>InsertUpdate</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dwh_db</connection>
    <commit>100</commit>
    <update_bypassed>N</update_bypassed>
    <lookup>
      <schema>passage</schema>
      <table>passage</table>
      <key>
        <name>passage_id</name>
        <field>passage_id</field>
        <condition>=</condition>
        <name2 />
      </key>
      <value>
        <name>passage_id</name>
        <rename>passage_id</rename>
        <update>N</update>
      </value>
      <value>
        <name>passage_status_id</name>
        <rename>passage_status_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>component_id</name>
        <rename>component_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>passage_date</name>
        <rename>passage_date</rename>
        <update>Y</update>
      </value>
      <value>
        <name>arrival_date</name>
        <rename>arrival_date</rename>
        <update>Y</update>
      </value>
      <value>
        <name>finished_date</name>
        <rename>finished_date</rename>
        <update>Y</update>
      </value>
      <value>
        <name>sent_date</name>
        <rename>sent_date</rename>
        <update>Y</update>
      </value>
      <value>
        <name>service_provider_id</name>
        <rename>service_provider_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>charging_point_id</name>
        <rename>charging_point_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>lane_id</name>
        <rename>lane_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>vehicle_direction_id</name>
        <rename>vehicle_direction_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>image_count</name>
        <rename>image_count</rename>
        <update>Y</update>
      </value>
      <value>
        <name>obu_count</name>
        <rename>obu_count</rename>
        <update>Y</update>
      </value>
      <value>
        <name>pcs_rate</name>
        <rename>pcs_rate</rename>
        <update>Y</update>
      </value>
      <value>
        <name>avc_class_id</name>
        <rename>avc_class_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>passage_priority</name>
        <rename>passage_priority</rename>
        <update>Y</update>
      </value>
      <value>
        <name>passage_type_id</name>
        <rename>passage_type_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>external_passage_id</name>
        <rename>external_passage_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>passage_group_id</name>
        <rename>passage_group_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>propertybag</name>
        <rename>propertybag</rename>
        <update>Y</update>
      </value>
      <value>
        <name>completion_category_id</name>
        <rename>completion_category_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>completion_sub_category_id</name>
        <rename>completion_sub_category_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>archive_id</name>
        <rename>archive_id</rename>
        <update>N</update>
      </value>
      <value>
        <name>archive_date</name>
        <rename>archive_date</rename>
        <update>N</update>
      </value>
      <value>
        <name>etl_batch_id_insert</name>
        <rename>etl_batch_id_insert</rename>
        <update>N</update>
      </value>
    </lookup>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>416</xloc>
      <yloc>512</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Table input -  TDP.passage.passage</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>obo_opr_db</connection>
    <sql>SELECT
    *
FROM
    passage.passage
WHERE
    archive_id > ? AND
    --archive_date >  '${PARAM_CDC_LAST_LOAD_UTC}' AND 
	archive_date &lt;= '${PARAM_CDC_CURRENT_LOAD_UTC}'
ORDER BY
    archive_id
--LIMIT 5000</sql>
    <limit>0</limit>
    <lookup>Write to log: parameters, max_target_insert_id</lookup>
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>112</xloc>
      <yloc>336</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Table output - DWH.passage.passage</name>
    <type>TableOutput</type>
    <description />
    <distribute>N</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dwh_db</connection>
    <schema>passage</schema>
    <table>passage</table>
    <commit>1000</commit>
    <truncate>N</truncate>
    <ignore_errors>N</ignore_errors>
    <use_batch>Y</use_batch>
    <specify_fields>N</specify_fields>
    <partitioning_enabled>N</partitioning_enabled>
    <partitioning_field />
    <partitioning_daily>N</partitioning_daily>
    <partitioning_monthly>Y</partitioning_monthly>
    <tablename_in_field>N</tablename_in_field>
    <tablename_field />
    <tablename_in_table>Y</tablename_in_table>
    <return_keys>N</return_keys>
    <return_field />
    <fields>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>112</xloc>
      <yloc>512</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Table output - DWH.passage.passage 2</name>
    <type>TableOutput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dwh_db</connection>
    <schema>passage</schema>
    <table>passage</table>
    <commit>1000</commit>
    <truncate>N</truncate>
    <ignore_errors>N</ignore_errors>
    <use_batch>N</use_batch>
    <specify_fields>N</specify_fields>
    <partitioning_enabled>N</partitioning_enabled>
    <partitioning_field />
    <partitioning_daily>N</partitioning_daily>
    <partitioning_monthly>Y</partitioning_monthly>
    <tablename_in_field>N</tablename_in_field>
    <tablename_field />
    <tablename_in_table>Y</tablename_in_table>
    <return_keys>N</return_keys>
    <return_field />
    <fields>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>704</xloc>
      <yloc>512</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Update - DWH.passage.passage</name>
    <type>Update</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dwh_db</connection>
    <skip_lookup>N</skip_lookup>
    <commit>100</commit>
    <use_batch>N</use_batch>
    <error_ignored>N</error_ignored>
    <ignore_flag_field />
    <lookup>
      <schema>passage</schema>
      <table>passage</table>
      <key>
        <name>passage_id</name>
        <field>passage_id</field>
        <condition>=</condition>
        <name2 />
      </key>
      <value>
        <name>passage_status_id</name>
        <rename>passage_status_id</rename>
      </value>
      <value>
        <name>component_id</name>
        <rename>component_id</rename>
      </value>
      <value>
        <name>passage_date</name>
        <rename>passage_date</rename>
      </value>
      <value>
        <name>arrival_date</name>
        <rename>arrival_date</rename>
      </value>
      <value>
        <name>finished_date</name>
        <rename>finished_date</rename>
      </value>
      <value>
        <name>sent_date</name>
        <rename>sent_date</rename>
      </value>
      <value>
        <name>service_provider_id</name>
        <rename>service_provider_id</rename>
      </value>
      <value>
        <name>charging_point_id</name>
        <rename>charging_point_id</rename>
      </value>
      <value>
        <name>lane_id</name>
        <rename>lane_id</rename>
      </value>
      <value>
        <name>vehicle_direction_id</name>
        <rename>vehicle_direction_id</rename>
      </value>
      <value>
        <name>image_count</name>
        <rename>image_count</rename>
      </value>
      <value>
        <name>obu_count</name>
        <rename>obu_count</rename>
      </value>
      <value>
        <name>pcs_rate</name>
        <rename>pcs_rate</rename>
      </value>
      <value>
        <name>avc_class_id</name>
        <rename>avc_class_id</rename>
      </value>
      <value>
        <name>passage_priority</name>
        <rename>passage_priority</rename>
      </value>
      <value>
        <name>passage_type_id</name>
        <rename>passage_type_id</rename>
      </value>
      <value>
        <name>external_passage_id</name>
        <rename>external_passage_id</rename>
      </value>
      <value>
        <name>passage_group_id</name>
        <rename>passage_group_id</rename>
      </value>
      <value>
        <name>propertybag</name>
        <rename>propertybag</rename>
      </value>
      <value>
        <name>completion_category_id</name>
        <rename>completion_category_id</rename>
      </value>
      <value>
        <name>completion_sub_category_id</name>
        <rename>completion_sub_category_id</rename>
      </value>
    </lookup>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>704</xloc>
      <yloc>704</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Write to log: parameters, max_target_insert_id</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_basic</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>PARAM_CDC_LAST_LOAD_UTC	= ${PARAM_CDC_LAST_LOAD_UTC}
PARAM_CDC_CURRENT_LOAD_UTC	= ${PARAM_CDC_CURRENT_LOAD_UTC}</logmessage>
    <fields>
      <field>
        <name>max_target_insert_id</name>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>112</xloc>
      <yloc>224</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step_error_handling>
    <error>
      <source_step>Table output - DWH.passage.passage 2</source_step>
      <target_step>Dummy</target_step>
      <is_enabled>Y</is_enabled>
      <nr_valuename />
      <descriptions_valuename />
      <fields_valuename />
      <codes_valuename />
      <max_errors />
      <max_pct_errors />
      <min_pct_rows />
    </error>
  </step_error_handling>
  <slave-step-copy-partition-distribution>
  </slave-step-copy-partition-distribution>
  <slave_transformation>N</slave_transformation>
</transformation>
