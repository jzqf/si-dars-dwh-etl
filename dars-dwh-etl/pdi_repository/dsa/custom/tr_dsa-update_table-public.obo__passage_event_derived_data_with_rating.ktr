<transformation>
  <info>
    <name>tr_dsa-update_table-public.obo__passage_event_derived_data_with_rating</name>
    <description />
    <extended_description />
    <trans_version />
    <trans_type>Normal</trans_type>
    <trans_status>0</trans_status>
    <directory>/dsa/custom</directory>
    <parameters>
      <parameter>
        <name>PARAM_CDC_CURRENT_LOAD_UTC</name>
        <default_value>2100.01.01 00:00:00</default_value>
        <description>Timestamp of the current load (no rows after this timestamp will be loaded)</description>
      </parameter>
      <parameter>
        <name>PARAM_CDC_LAST_LOAD_UTC</name>
        <default_value>1970.01.01 00:00:00</default_value>
        <description>Timestamp of last successful load from the source database</description>
      </parameter>
      <parameter>
        <name>PARAM_SOURCE_SCHEMA</name>
        <default_value>public</default_value>
        <description>Name of schema containing the "main" source table to mirror to target table</description>
      </parameter>
      <parameter>
        <name>PARAM_SOURCE_TABLE</name>
        <default_value>obo__passage_event_derived_data</default_value>
        <description>Name of the "main" source table to mirror to target table</description>
      </parameter>
      <parameter>
        <name>PARAM_SOURCE_TABLE_MAX_INSERT_ID</name>
        <default_value>1964240</default_value>
        <description>Maximum value of the "insert_id" column for rows to load from the source DB table</description>
      </parameter>
      <parameter>
        <name>PARAM_TARGET_SCHEMA</name>
        <default_value>public</default_value>
        <description>Name of schema containing target table to update</description>
      </parameter>
      <parameter>
        <name>PARAM_TARGET_TABLE</name>
        <default_value>obo__passage_event_derived_data_with_rating</default_value>
        <description>Name of target table to update</description>
      </parameter>
    </parameters>
    <log>
      <trans-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANS_SCHEMA}</schema>
        <table>${QF_LOG_TRANS_TABLE}</table>
        <size_limit_lines />
        <interval />
        <timeout_days>${QF_LOG_TRANS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STATUS</id>
          <enabled>Y</enabled>
          <name>STATUS</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
          <subject>Table output - DSA.schema.table 2</subject>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
          <subject>Table output - DSA.schema.table 2</subject>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
          <subject>Table output - DSA.schema.table 2</subject>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
          <subject>Table input - source PSA tables</subject>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
          <subject>Table output - DSA.schema.table 2</subject>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
          <subject>Table output - DSA.schema.table 2</subject>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>STARTDATE</id>
          <enabled>Y</enabled>
          <name>STARTDATE</name>
        </field>
        <field>
          <id>ENDDATE</id>
          <enabled>Y</enabled>
          <name>ENDDATE</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>DEPDATE</id>
          <enabled>Y</enabled>
          <name>DEPDATE</name>
        </field>
        <field>
          <id>REPLAYDATE</id>
          <enabled>Y</enabled>
          <name>REPLAYDATE</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>Y</enabled>
          <name>LOG_FIELD</name>
        </field>
        <field>
          <id>EXECUTING_SERVER</id>
          <enabled>N</enabled>
          <name>EXECUTING_SERVER</name>
        </field>
        <field>
          <id>EXECUTING_USER</id>
          <enabled>N</enabled>
          <name>EXECUTING_USER</name>
        </field>
        <field>
          <id>CLIENT</id>
          <enabled>N</enabled>
          <name>CLIENT</name>
        </field>
      </trans-log-table>
      <perf-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANSPERF_SCHEMA}</schema>
        <table>${QF_LOG_TRANSPERF_TABLE}</table>
        <interval />
        <timeout_days>${QF_LOG_TRANSPERF_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>SEQ_NR</id>
          <enabled>Y</enabled>
          <name>SEQ_NR</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>INPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>INPUT_BUFFER_ROWS</name>
        </field>
        <field>
          <id>OUTPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>OUTPUT_BUFFER_ROWS</name>
        </field>
      </perf-log-table>
      <channel-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_CHANNEL_SCHEMA}</schema>
        <table>${QF_LOG_CHANNEL_TABLE}</table>
        <timeout_days>${QF_LOG_CHANNEL_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>LOGGING_OBJECT_TYPE</id>
          <enabled>Y</enabled>
          <name>LOGGING_OBJECT_TYPE</name>
        </field>
        <field>
          <id>OBJECT_NAME</id>
          <enabled>Y</enabled>
          <name>OBJECT_NAME</name>
        </field>
        <field>
          <id>OBJECT_COPY</id>
          <enabled>Y</enabled>
          <name>OBJECT_COPY</name>
        </field>
        <field>
          <id>REPOSITORY_DIRECTORY</id>
          <enabled>Y</enabled>
          <name>REPOSITORY_DIRECTORY</name>
        </field>
        <field>
          <id>FILENAME</id>
          <enabled>Y</enabled>
          <name>FILENAME</name>
        </field>
        <field>
          <id>OBJECT_ID</id>
          <enabled>Y</enabled>
          <name>OBJECT_ID</name>
        </field>
        <field>
          <id>OBJECT_REVISION</id>
          <enabled>Y</enabled>
          <name>OBJECT_REVISION</name>
        </field>
        <field>
          <id>PARENT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>PARENT_CHANNEL_ID</name>
        </field>
        <field>
          <id>ROOT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>ROOT_CHANNEL_ID</name>
        </field>
      </channel-log-table>
      <step-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANSSTEP_SCHEMA}</schema>
        <table>${QF_LOG_TRANSSTEP_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSSTEP_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>N</enabled>
          <name>LOG_FIELD</name>
        </field>
      </step-log-table>
      <metrics-log-table>
        <connection>${QF_LOGGING_DB_CONNECTION}</connection>
        <schema>${QF_LOG_TRANSMETRICS_SCHEMA}</schema>
        <table>${QF_LOG_TRANSMETRICS_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSMETRICS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>METRICS_DATE</id>
          <enabled>Y</enabled>
          <name>METRICS_DATE</name>
        </field>
        <field>
          <id>METRICS_CODE</id>
          <enabled>Y</enabled>
          <name>METRICS_CODE</name>
        </field>
        <field>
          <id>METRICS_DESCRIPTION</id>
          <enabled>Y</enabled>
          <name>METRICS_DESCRIPTION</name>
        </field>
        <field>
          <id>METRICS_SUBJECT</id>
          <enabled>Y</enabled>
          <name>METRICS_SUBJECT</name>
        </field>
        <field>
          <id>METRICS_TYPE</id>
          <enabled>Y</enabled>
          <name>METRICS_TYPE</name>
        </field>
        <field>
          <id>METRICS_VALUE</id>
          <enabled>Y</enabled>
          <name>METRICS_VALUE</name>
        </field>
      </metrics-log-table>
    </log>
    <maxdate>
      <connection />
      <table />
      <field />
      <offset>0.0</offset>
      <maxdiff>0.0</maxdiff>
    </maxdate>
    <size_rowset>10000</size_rowset>
    <sleep_time_empty>50</sleep_time_empty>
    <sleep_time_full>50</sleep_time_full>
    <unique_connections>N</unique_connections>
    <feedback_shown>Y</feedback_shown>
    <feedback_size>50000</feedback_size>
    <using_thread_priorities>Y</using_thread_priorities>
    <shared_objects_file />
    <capture_step_performance>Y</capture_step_performance>
    <step_performance_capturing_delay>1000</step_performance_capturing_delay>
    <step_performance_capturing_size_limit>100</step_performance_capturing_size_limit>
    <dependencies>
    </dependencies>
    <partitionschemas>
    </partitionschemas>
    <slaveservers>
    </slaveservers>
    <clusterschemas>
    </clusterschemas>
    <created_user>-</created_user>
    <created_date>2016/04/03 09:57:16.279</created_date>
    <modified_user>-</modified_user>
    <modified_date>2018/05/28 08:43:59.285</modified_date>
    <key_for_session_key>H4sIAAAAAAAAAAMAAAAAAAAAAAA=</key_for_session_key>
    <is_key_private>N</is_key_private>
  </info>
  <notepads>
    <notepad>
      <note>Load the data from the source tables that will be moved to the target DSA table.

It is important to sort these rows chronologically (by either the "insert_id" or "inserted_on" columns) so that if this 
transformation fails in the middle somewhere and if this transaction does not run in a single transaction, then next time 
this transformation is executed, we can start again where we left off (by computing, e.g., MAX(insert_id) for the DSA table).</note>
      <xloc>608</xloc>
      <yloc>320</yloc>
      <width>688</width>
      <heigth>75</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Insert new rows into the DSA using a "Table output" step. Error handing is enabled for this step so that
rows already exist in the target table (based on primary key) are passed on to an "Update" step.

IMPORTANT:	A primary key constraint (or at least a "unique" constraint on the primary key column(s)) 
				for the target table is *required*; otherwise, error handling will not be triggered when 
				attempting to insert a row with a duplicate key.

For benchmark results I obtained testing the speed of various INSERT/UPDATE algorithms, see: 

	/development/jeffreyz/tests/cdc_timestamps/tr_TEST_mirror_update-passage.passage-table_output</note>
      <xloc>736</xloc>
      <yloc>944</yloc>
      <width>588</width>
      <heigth>153</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Get batch ID for current transformation. This will be stored in the DSA table for auditing/logging.
It *must* be named "etl_batch_id_insert" here because we do not specify the table / stream fields in the "Table output" step.</note>
      <xloc>608</xloc>
      <yloc>416</yloc>
      <width>696</width>
      <heigth>36</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>This transformation implements a custom algorithm to update the DSA table:

	public.obo__passage_event_derived_data_with_rating .

This transformation has parameters:

	PARAM_CDC_LAST_LOAD_UTC			Timestamp for when *all* tables of the DSA were most recently updated successfully
	PARAM_CDC_CURRENT_LOAD_UTC		Used so that a consistent max timestamp is used for all ETL
	PARAM_SOURCE_TABLE_MAX_INSERT_ID	Maximum value of the "insert_id" column for rows to load from the source DB table
	PARAM_SOURCE_SCHEMA				Name of schema containing the "main" source table to archive/mirror to target table
	PARAM_SOURCE_TABLE					Name of the "main" source table to archive/mirror to target table
	PARAM_TARGET_SCHEMA				Name of schema containing target table to update
	PARAM_TARGET_TABLE					Name of target table to update</note>
      <xloc>0</xloc>
      <yloc>0</yloc>
      <width>740</width>
      <heigth>179</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>Get the maximum values stored in the "insert_id" &amp; various timestamp columns of the target table that will be updated by this 
transformation. This will be used to the select data from the source table that was inserted or modified since the last update 
to the target DSA DB.</note>
      <xloc>608</xloc>
      <yloc>192</yloc>
      <width>702</width>
      <heigth>49</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>A primary key 
constraint or a
unique constraint
on primary key
column(s) is 


*required* in 
order for error 
handling to be 
triggered here!</note>
      <xloc>1168</xloc>
      <yloc>480</yloc>
      <width>108</width>
      <heigth>164</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>Y</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>255</backgroundcolorgreen>
      <backgroundcolorblue>0</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
When using a *local* target database, 
this algorithm is *slower" than using 
the "Table output" step with error 
handling to an "Update" step. But it 
is *faster* for a *remote* target 
database!</note>
      <xloc>0</xloc>
      <yloc>832</yloc>
      <width>219</width>
      <heigth>101</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This algorithm appears to be fastest for both local and remote target 
databases. But because of the potential race condition described 
above, be careful that it is only used where this in not an issue.</note>
      <xloc>272</xloc>
      <yloc>1120</yloc>
      <width>392</width>
      <heigth>62</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>The "insert_if_null" field is set to the matching value of 
pedd_id for rows that already exist in the target table, 
and it is set to -1 for rows that do *not* exist in the 
target table. INSERTs are executed below if 
insert_if_null=-1, and UPDATEs are executed otherwise.
There is a potential race condition because the data in 
the target table can theoretically be modified between 
the time that we create the "insert_if_null" field here 
and we execute the INSERTs or UPDATEs below, but for
the current use case, this job is the only way that the 
target table can be modified so we do *not* need to 
worry about this potential race condition.</note>
      <xloc>544</xloc>
      <yloc>480</yloc>
      <width>317</width>
      <heigth>166</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>9</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <connection>
    <name>dsa_db</name>
    <server>${QF_DSA_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_DSA_DB_DATABASE}</database>
    <port>${QF_DSA_DB_PORT}</port>
    <username>${QF_DSA_DB_USERNAME}</username>
    <password>${QF_DSA_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_DSA_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <connection>
    <name>logging_db</name>
    <server>${QF_LOGGING_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_LOGGING_DB_DATABASE}</database>
    <port>${QF_LOGGING_DB_PORT}</port>
    <username>${QF_LOGGING_DB_USERNAME}</username>
    <password>${QF_LOGGING_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_LOGGING_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <connection>
    <name>psa_db</name>
    <server>${QF_PSA_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_PSA_DB_DATABASE}</database>
    <port>${QF_PSA_DB_PORT}</port>
    <username>${QF_PSA_DB_USERNAME}</username>
    <password>${QF_PSA_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_PSA_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <order>
    <hop>
      <from>Get max id &amp; timestamps for target DSA table</from>
      <to>Write to log: parameters, max id &amp; timestamps</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Write to log: parameters, max id &amp; timestamps</from>
      <to>Table input - source PSA tables</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Set etl_batch_id_insert = Xform batch ID</from>
      <to>Table output - DSA.schema.table 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Table input - source PSA tables</from>
      <to>Set etl_batch_id_insert = Xform batch ID</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Set insert_if_null flag field</from>
      <to>INSERT or UPDATE?</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Set etl_batch_id_insert = Xform batch ID</from>
      <to>Set insert_if_null flag field</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>INSERT or UPDATE?</from>
      <to>Perform INSERT</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>INSERT or UPDATE?</from>
      <to>Perform UPDATE</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Perform UPDATE</from>
      <to>Update - DSA.schema.table</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Perform INSERT</from>
      <to>Remove field "insert_if_null"</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Remove field "insert_if_null"</from>
      <to>Table output - DSA.schema.table</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Set etl_batch_id_insert = Xform batch ID</from>
      <to>Insert / Update- DSA.schema.table</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Table output - DSA.schema.table 2</from>
      <to>Update - DSA.schema.table 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Compute number of rows updated</from>
      <to>Set QF_UPDATE_TABLE_NUM_UPDATES</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Compute number of rows processed</from>
      <to>Set QF_UPDATE_TABLE_NUM_ROWS</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Insert / Update- DSA.schema.table</from>
      <to>Compute number of rows processed</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Update - DSA.schema.table</from>
      <to>Compute number of rows updated</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Compute number of rows inserted</from>
      <to>Set QF_UPDATE_TABLE_NUM_INSERTS</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Table output - DSA.schema.table</from>
      <to>Compute number of rows inserted</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Compute number of rows updated 2</from>
      <to>Set QF_UPDATE_TABLE_NUM_UPDATES 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Update - DSA.schema.table 2</from>
      <to>Compute number of rows updated 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Compute number of rows processed 2</from>
      <to>Set QF_UPDATE_TABLE_NUM_ROWS 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Table input - source PSA tables</from>
      <to>Compute number of rows processed 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Set QF_UPDATE_TABLE_NUM_INSERTS</from>
      <to>Get transformation name</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Get transformation name</from>
      <to>Log number of rows inserted</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Set QF_UPDATE_TABLE_NUM_UPDATES</from>
      <to>Get transformation name 2</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Get transformation name 2</from>
      <to>Log number of rows updated</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Set QF_UPDATE_TABLE_NUM_ROWS</from>
      <to>Get transformation name 3</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Get transformation name 3</from>
      <to>Log number of rows processed</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Set QF_UPDATE_TABLE_NUM_ROWS 2</from>
      <to>Get transformation name 4</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Get transformation name 4</from>
      <to>Log number of rows processed 2</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Set QF_UPDATE_TABLE_NUM_UPDATES 2</from>
      <to>Get transformation name 5</to>
      <enabled>N</enabled>
    </hop>
    <hop>
      <from>Get transformation name 5</from>
      <to>Log number of rows updated 2</to>
      <enabled>N</enabled>
    </hop>
  </order>
  <step>
    <name>Compute number of rows inserted</name>
    <type>MemoryGroupBy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <give_back_row>Y</give_back_row>
    <group>
      </group>
    <fields>
      <field>
        <aggregate>num_rows_inserted</aggregate>
        <subject />
        <type>COUNT_ANY</type>
        <valuefield />
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>336</xloc>
      <yloc>864</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Compute number of rows processed</name>
    <type>MemoryGroupBy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <give_back_row>Y</give_back_row>
    <group>
      </group>
    <fields>
      <field>
        <aggregate>num_rows</aggregate>
        <subject />
        <type>COUNT_ANY</type>
        <valuefield />
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>96</xloc>
      <yloc>576</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Compute number of rows processed 2</name>
    <type>MemoryGroupBy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <give_back_row>Y</give_back_row>
    <group>
      </group>
    <fields>
      <field>
        <aggregate>num_rows</aggregate>
        <subject />
        <type>COUNT_ANY</type>
        <valuefield />
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>816</xloc>
      <yloc>688</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Compute number of rows updated</name>
    <type>MemoryGroupBy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <give_back_row>Y</give_back_row>
    <group>
      </group>
    <fields>
      <field>
        <aggregate>num_rows_updated</aggregate>
        <subject />
        <type>COUNT_ANY</type>
        <valuefield />
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>560</xloc>
      <yloc>864</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Compute number of rows updated 2</name>
    <type>MemoryGroupBy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <give_back_row>Y</give_back_row>
    <group>
      </group>
    <fields>
      <field>
        <aggregate>num_rows_updated</aggregate>
        <subject />
        <type>COUNT_ANY</type>
        <valuefield />
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>1056</xloc>
      <yloc>688</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get max id &amp; timestamps for target DSA table</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dsa_db</connection>
    <sql>SELECT
    COALESCE(MAX(pedd_id),                ${QF_INSERT_ID_LOWER_BOUND}       ) AS "max_target_insert_id_1", 
    COALESCE(MAX(pedd_id),                ${QF_INSERT_ID_LOWER_BOUND}       ) AS "max_target_insert_id_2", 
--  COALESCE(MAX(created_on),            '${QF_LAST_UPDATED_ON_LOWER_BOUND}') AS "max_target_created_on", 
    COALESCE(MAX(last_updated_on),       '${QF_LAST_UPDATED_ON_LOWER_BOUND}') AS "max_target_last_updated_on", 
    COALESCE(MAX(peddd_created_on),      '${QF_LAST_UPDATED_ON_LOWER_BOUND}') AS "max_target_peddd_created_on", 
    COALESCE(MAX(peddd_last_updated_on), '${QF_LAST_UPDATED_ON_LOWER_BOUND}') AS "max_target_peddd_last_updated_on" 
FROM
    public.obo__passage_event_derived_data_with_rating  -- ${PARAM_TARGET_SCHEMA}.${PARAM_TARGET_TABLE}</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>448</xloc>
      <yloc>208</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get transformation name</name>
    <type>SystemInfo</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <name>current_transformation</name>
        <type>transformation name</type>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>336</xloc>
      <yloc>992</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get transformation name 2</name>
    <type>SystemInfo</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <name>current_transformation</name>
        <type>transformation name</type>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>560</xloc>
      <yloc>992</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get transformation name 3</name>
    <type>SystemInfo</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <name>current_transformation</name>
        <type>transformation name</type>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>96</xloc>
      <yloc>704</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get transformation name 4</name>
    <type>SystemInfo</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <name>current_transformation</name>
        <type>transformation name</type>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>816</xloc>
      <yloc>816</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Get transformation name 5</name>
    <type>SystemInfo</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <name>current_transformation</name>
        <type>transformation name</type>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>1056</xloc>
      <yloc>816</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>INSERT or UPDATE?</name>
    <type>FilterRows</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <send_true_to>Perform INSERT</send_true_to>
    <send_false_to>Perform UPDATE</send_false_to>
    <compare>
      <condition>
        <negated>N</negated>
        <leftvalue>insert_if_null</leftvalue>
        <function>=</function>
        <rightvalue />
        <value>
          <name>constant</name>
          <type>Integer</type>
          <text>-1</text>
          <length>-1</length>
          <precision>0</precision>
          <isnull>N</isnull>
          <mask>####0;-####0</mask>
        </value>
      </condition>
    </compare>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>448</xloc>
      <yloc>576</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Insert / Update- DSA.schema.table</name>
    <type>InsertUpdate</type>
    <description />
    <distribute>N</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dsa_db</connection>
    <commit>100</commit>
    <update_bypassed>N</update_bypassed>
    <lookup>
      <schema>${PARAM_TARGET_SCHEMA}</schema>
      <table>${PARAM_TARGET_TABLE}</table>
      <key>
        <name>pedd_id</name>
        <field>pedd_id</field>
        <condition>=</condition>
        <name2 />
      </key>
      <value>
        <name>pedd_id</name>
        <rename>pedd_id</rename>
        <update>N</update>
      </value>
      <value>
        <name>pedd_status_id</name>
        <rename>pedd_status_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_axle_tariff_category_id</name>
        <rename>applied_axle_tariff_category_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_euro_emission_class_id</name>
        <rename>applied_euro_emission_class_id</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_lpn_country_code</name>
        <rename>applied_lpn_country_code</rename>
        <update>Y</update>
      </value>
      <value>
        <name>base_rate_total</name>
        <rename>base_rate_total</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_infrastructure_fee</name>
        <rename>applied_infrastructure_fee</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_infrastructure_vat</name>
        <rename>applied_infrastructure_vat</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_surcharge</name>
        <rename>applied_surcharge</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_surcharge_vat</name>
        <rename>applied_surcharge_vat</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_external_fee_noise</name>
        <rename>applied_external_fee_noise</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_external_fee_noise_vat</name>
        <rename>applied_external_fee_noise_vat</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_external_fee_air</name>
        <rename>applied_external_fee_air</rename>
        <update>Y</update>
      </value>
      <value>
        <name>applied_external_fee_air_vat</name>
        <rename>applied_external_fee_air_vat</rename>
        <update>Y</update>
      </value>
      <value>
        <name>created_on</name>
        <rename>created_on</rename>
        <update>Y</update>
      </value>
      <value>
        <name>last_updated_on</name>
        <rename>last_updated_on</rename>
        <update>Y</update>
      </value>
      <value>
        <name>peddd_created_on</name>
        <rename>peddd_created_on</rename>
        <update>Y</update>
      </value>
      <value>
        <name>peddd_last_updated_on</name>
        <rename>peddd_last_updated_on</rename>
        <update>Y</update>
      </value>
      <value>
        <name>etl_batch_id_insert</name>
        <rename>etl_batch_id_insert</rename>
        <update>N</update>
      </value>
      <value>
        <name>etl_batch_id_last_update</name>
        <rename>etl_batch_id_insert</rename>
        <update>Y</update>
      </value>
    </lookup>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>96</xloc>
      <yloc>512</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Log number of rows inserted</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_minimal</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>PARAM_SOURCE_SCHEMA = ${PARAM_SOURCE_SCHEMA}
PARAM_SOURCE_TABLE  = ${PARAM_SOURCE_TABLE}
PARAM_TARGET_SCHEMA = ${PARAM_TARGET_SCHEMA}
PARAM_TARGET_TABLE  = ${PARAM_TARGET_TABLE}</logmessage>
    <fields>
      <field>
        <name>current_transformation</name>
      </field>
      <field>
        <name>num_rows_inserted</name>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>336</xloc>
      <yloc>1056</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Log number of rows processed</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_minimal</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>PARAM_SOURCE_SCHEMA = ${PARAM_SOURCE_SCHEMA}
PARAM_SOURCE_TABLE  = ${PARAM_SOURCE_TABLE}
PARAM_TARGET_SCHEMA = ${PARAM_TARGET_SCHEMA}
PARAM_TARGET_TABLE  = ${PARAM_TARGET_TABLE}</logmessage>
    <fields>
      <field>
        <name>current_transformation</name>
      </field>
      <field>
        <name>num_rows</name>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>96</xloc>
      <yloc>768</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Log number of rows processed 2</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_minimal</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>PARAM_SOURCE_SCHEMA = ${PARAM_SOURCE_SCHEMA}
PARAM_SOURCE_TABLE  = ${PARAM_SOURCE_TABLE}
PARAM_TARGET_SCHEMA = ${PARAM_TARGET_SCHEMA}
PARAM_TARGET_TABLE  = ${PARAM_TARGET_TABLE}</logmessage>
    <fields>
      <field>
        <name>current_transformation</name>
      </field>
      <field>
        <name>num_rows</name>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>816</xloc>
      <yloc>880</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Log number of rows updated</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_minimal</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>PARAM_SOURCE_SCHEMA = ${PARAM_SOURCE_SCHEMA}
PARAM_SOURCE_TABLE  = ${PARAM_SOURCE_TABLE}
PARAM_TARGET_SCHEMA = ${PARAM_TARGET_SCHEMA}
PARAM_TARGET_TABLE  = ${PARAM_TARGET_TABLE}</logmessage>
    <fields>
      <field>
        <name>current_transformation</name>
      </field>
      <field>
        <name>num_rows_updated</name>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>560</xloc>
      <yloc>1056</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Log number of rows updated 2</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_minimal</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>PARAM_SOURCE_SCHEMA = ${PARAM_SOURCE_SCHEMA}
PARAM_SOURCE_TABLE  = ${PARAM_SOURCE_TABLE}
PARAM_TARGET_SCHEMA = ${PARAM_TARGET_SCHEMA}
PARAM_TARGET_TABLE  = ${PARAM_TARGET_TABLE}</logmessage>
    <fields>
      <field>
        <name>current_transformation</name>
      </field>
      <field>
        <name>num_rows_updated</name>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>1056</xloc>
      <yloc>880</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Perform INSERT</name>
    <type>Dummy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>336</xloc>
      <yloc>672</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Perform UPDATE</name>
    <type>Dummy</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>560</xloc>
      <yloc>672</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Remove field "insert_if_null"</name>
    <type>SelectValues</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <select_unspecified>N</select_unspecified>
      <remove>
        <name>insert_if_null</name>
      </remove>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>336</xloc>
      <yloc>736</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Set QF_UPDATE_TABLE_NUM_INSERTS</name>
    <type>SetVariable</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <field_name>num_rows_inserted</field_name>
        <variable_name>QF_UPDATE_TABLE_NUM_INSERTS</variable_name>
        <variable_type>ROOT_JOB</variable_type>
        <default_value />
      </field>
    </fields>
    <use_formatting>Y</use_formatting>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>336</xloc>
      <yloc>928</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Set QF_UPDATE_TABLE_NUM_ROWS</name>
    <type>SetVariable</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <field_name>num_rows</field_name>
        <variable_name>QF_UPDATE_TABLE_NUM_ROWS</variable_name>
        <variable_type>ROOT_JOB</variable_type>
        <default_value />
      </field>
    </fields>
    <use_formatting>Y</use_formatting>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>96</xloc>
      <yloc>640</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Set QF_UPDATE_TABLE_NUM_ROWS 2</name>
    <type>SetVariable</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <field_name>num_rows</field_name>
        <variable_name>QF_UPDATE_TABLE_NUM_ROWS</variable_name>
        <variable_type>ROOT_JOB</variable_type>
        <default_value />
      </field>
    </fields>
    <use_formatting>Y</use_formatting>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>816</xloc>
      <yloc>752</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Set QF_UPDATE_TABLE_NUM_UPDATES</name>
    <type>SetVariable</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <field_name>num_rows_updated</field_name>
        <variable_name>QF_UPDATE_TABLE_NUM_UPDATES</variable_name>
        <variable_type>ROOT_JOB</variable_type>
        <default_value />
      </field>
    </fields>
    <use_formatting>Y</use_formatting>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>560</xloc>
      <yloc>928</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Set QF_UPDATE_TABLE_NUM_UPDATES 2</name>
    <type>SetVariable</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <field_name>num_rows_updated</field_name>
        <variable_name>QF_UPDATE_TABLE_NUM_UPDATES</variable_name>
        <variable_type>ROOT_JOB</variable_type>
        <default_value />
      </field>
    </fields>
    <use_formatting>Y</use_formatting>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>1056</xloc>
      <yloc>752</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Set etl_batch_id_insert = Xform batch ID</name>
    <type>SystemInfo</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <fields>
      <field>
        <name>etl_batch_id_insert</name>
        <type>batch ID</type>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>448</xloc>
      <yloc>416</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Set insert_if_null flag field</name>
    <type>DBLookup</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dsa_db</connection>
    <cache>N</cache>
    <cache_load_all>N</cache_load_all>
    <cache_size>0</cache_size>
    <lookup>
      <schema>${PARAM_TARGET_SCHEMA}</schema>
      <table>${PARAM_TARGET_TABLE}</table>
      <orderby />
      <fail_on_multiple>N</fail_on_multiple>
      <eat_row_on_failure>N</eat_row_on_failure>
      <key>
        <name>pedd_id</name>
        <field>pedd_id</field>
        <condition>=</condition>
        <name2 />
      </key>
      <value>
        <name>pedd_id</name>
        <rename>insert_if_null</rename>
        <default>-1</default>
        <type>Integer</type>
      </value>
    </lookup>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>448</xloc>
      <yloc>496</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Table input - source PSA tables</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>psa_db</connection>
    <sql>WITH fully_linked_pedd AS 
(
    -- This query returns the maximum value of the "pedd_id" column for
    -- obo__passage_event_derived_data rows that are linked to all possible rows.
    -- This ensures that we do not try to migrate obo__passage_event_derived_data
    -- rows from the PSA DB to the DSA DB which are linked to rows of either tables:
    --
    --   * obo__passage_event_derived_data_details
    --   * obo__applied_rating_detail_data
    --
    -- in the obo_opr database, but where the obo__passage_event_derived_data_details
    -- rows and/or the obo__applied_rating_detail_data rows have not yet been migrated
    -- from the obo_opr database to the PSA DB ("late arriving" related rows). If such 
    -- obo__passage_event_derived_data rows where not excluded here, the result could be
    -- that some of the obo__passage_event_derived_data_with_rating rows that are 
    -- inserted into the DSA DB by this transformation will not have their columns  
    -- properly defined because the related rows that are need to to define these column
    -- values are not yet available in the PSA DB. These "missing"related rows will 
    -- eventually be migrated from the obo_opr databses to the PSA DB (on the next run
    -- of the ETL job), but there is no way to detect this after the 
    -- obo__passage_event_derived_data_with_rating row is created (this is particularly
    -- true of the obo__applied_rating_detail_data rows (because they do not participate 
    -- in the CDC implemented in this algorithm).
    -- the PSA DB that are migrated to the DSA DB (because they are not linked 
    -- to all of the rows enforced by this query).
    -- 
    -- Note: Event with this extra effort to avoid "late arriving" PSA DB
    --       obo__applied_rating_detail_data rows (or, equivalently, "early arriving"
    --       PSA DB obo__passage_event_derived_data (pedd) rows), there is still no 
    --       mechanism to discover PSA DB obo__applied_rating_detail_data rows that are
    --       inserted or updated *after" its obo__passage_event_derived_data_with_rating
    --       row is inserted into the DSA DB. In order to be able to detect this and 
    --       thereby be able to update an existing obo__passage_event_derived_data_with_rating
    --       row, the OBO must:
    --
    --       1. Support a "last_updated_on" column for the  set the applied_rating_detail_data
    --          table (assuming rows of this table can be updated). This is necesary so that
    --          changes to rwows of this table will be propagated to the PSA (archive) DB.
    --
    --       2. Update the "last_updated_on" column of the passage_event_derived_data (pedd) or 
    --          passage_event_derived_data_details (peddd) row for the applied_rating_detail_data
    --          row that is inserted or updated. 
    SELECT 
        COALESCE(MAX(pedd.pedd_id), ${QF_INSERT_ID_LOWER_BOUND}) AS "max_linked_pedd_id"
    FROM 
        public.obo__passage_event_derived_data pedd
    INNER JOIN
        public.obo__passage_event_derived_data_details peddd ON peddd.pedd_id=pedd.pedd_id
    INNER JOIN
        public.obo__applied_rating_detail_data ardd_1 ON ardd_1.pedd_id=peddd.pedd_id 
    INNER JOIN
        public.obo__applied_rating_detail_data ardd_2 ON ardd_2.pedd_id=peddd.pedd_id 
    INNER JOIN
        public.obo__applied_rating_detail_data ardd_4 ON ardd_4.pedd_id=peddd.pedd_id 
    INNER JOIN
        public.obo__applied_rating_detail_data ardd_3 ON ardd_3.pedd_id=peddd.pedd_id 
    WHERE
        pedd.pedd_id > ?      -- Newly INSERTed "pedd" rows,  since the last run.
        AND ardd_1.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_1.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_INFRASTRUCTURE_FEE}
        AND ardd_2.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_2.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_INFRASTRUCTURE_MARKUP_FEE}
        AND ardd_4.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_4.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_EXTERNAL_FEE_NOISE}
        AND ardd_3.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_3.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_EXTERNAL_FEE_AIR}
)
SELECT
    pedd.pedd_id,
    pedd.pedd_status_id,
    peddd.applied_axle_tariff_category_id,
    peddd.applied_euro_emission_class_id,
    peddd.applied_lpn_country AS "applied_lpn_country_code",
    peddd.base_rate_total,
    ardd_1.rate_component_fee_amount AS "applied_infrastructure_fee",
    ardd_1.rate_component_vat        AS "applied_infrastructure_vat",
    ardd_2.rate_component_fee_amount AS "applied_surcharge",
    ardd_2.rate_component_vat        AS "applied_surcharge_vat",
    ardd_4.rate_component_fee_amount AS "applied_external_fee_noise",
    ardd_4.rate_component_vat        AS "applied_external_fee_noise_vat",
    ardd_3.rate_component_fee_amount AS "applied_external_fee_air",
    ardd_3.rate_component_vat        AS "applied_external_fee_air_vat",
    pedd.created_on, 
    pedd.last_updated_on, 
    peddd.created_on                 AS "peddd_created_on",
    peddd.last_updated_on            AS "peddd_last_updated_on"
FROM
    public.obo__passage_event_derived_data pedd
LEFT OUTER JOIN
    public.obo__passage_event_derived_data_details peddd ON peddd.pedd_id=pedd.pedd_id  -- one-to-one relations, but we cannot be sure that a "peddd" row has been created yet
LEFT OUTER JOIN
    public.obo__applied_rating_detail_data ardd_1 ON ardd_1.pedd_id=peddd.pedd_id AND 
    ardd_1.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_1.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_INFRASTRUCTURE_FEE}
LEFT OUTER JOIN
    public.obo__applied_rating_detail_data ardd_2 ON ardd_2.pedd_id=peddd.pedd_id AND 
    ardd_2.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_2.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_INFRASTRUCTURE_MARKUP_FEE}
LEFT OUTER JOIN
    public.obo__applied_rating_detail_data ardd_4 ON ardd_4.pedd_id=peddd.pedd_id AND 
    ardd_4.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_4.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_EXTERNAL_FEE_NOISE}
LEFT OUTER JOIN
    public.obo__applied_rating_detail_data ardd_3 ON ardd_3.pedd_id=peddd.pedd_id AND 
    ardd_3.rate_component_fee_type_id=${QF_RATE_COMPONENT_FEE_TYPE_ID_APPLIED_FEE} AND ardd_3.rate_component_fee_category_id=${QF_RATE_COMPONENT_FEE_CATEGORY_ID_EXTERNAL_FEE_AIR}
WHERE
    -- This condition ensures that "pedd" rows selected here have no "late arriving" 
    -- rows from its related tables, "peddd" and "obo__applied_rating_detail_data":
    pedd.pedd_id &lt;= (SELECT max_linked_pedd_id FROM fully_linked_pedd) AND  -- Ensure that the most recent (last, chronologically) source pedd row treated here is "fully" linked to related rows
    ( 
           pedd.pedd_id          > ?      -- Newly INSERTed "pedd" rows,  since the last run.
        -- The >= comparison operator is used here because timestamps are not unique.
        -- As a result, these comparisons will always match at least one row:
        OR pedd.last_updated_on  >= ?    -- UPDATEd        "pedd" rows,  since the last run.
        OR peddd.created_on      >= ?    -- Newly INSERTed "peddd" rows, since the last run.
                                         -- This condition is based on peddd.created_on, not peddd.pedd_id because I do not know for sure that peddd rows are
                                         -- created simultaneously as each pedd row is created. If this is not the case, then the peddd.pedd_id column *cannot*
                                         -- be used as the "insert_id" column for the peddd table because we cannot assume that the peddd.pedd_id increase
                                         -- monotonically as the peddd rows are inserted. Therefore, I have been forced to detect newly inserted
                                         -- peddd rows using the peddd.created_on column. If it can be confirmed that the peddd.pedd_id column *can* be treated
                                         -- as the "insert_id" column for the peddd table, then it is desirable to base this condition on the peddd.pedd_id
                                         -- column instead of the peddd.created_on column
        OR peddd.last_updated_on >= ?    -- UPDATEd        "peddd" rows, since the last run.
-- Old code:
--        OR pedd.last_updated_on  > '${PARAM_CDC_LAST_LOAD_UTC}'      -- UPDATEd        "pedd" rows,  since the last run.
--        OR peddd.created_on      > '${PARAM_CDC_LAST_LOAD_UTC}'      -- Newly INSERTed "peddd" rows, since the last run.
--        OR peddd.last_updated_on > '${PARAM_CDC_LAST_LOAD_UTC}'      -- UPDATEd        "peddd" rows, since the last run.
    )
    AND pedd.pedd_id  &lt;= ${PARAM_SOURCE_TABLE_MAX_INSERT_ID}          -- To avoid loading rows that are *inserted* after the job starts.
                                                         -- It is not necessary to include a similar condition for the "peddd"
                                                         -- table because it will necessarily be satisfied, due to 
                                                         -- the one-to-one relation between the "pedd" &amp; "peddd" tables.
-- This condition is commented out because it is not needed and leads to problems if used.
-- For details, see the comments in the step that creates the SELECT statement for the source table
-- in transformation:
--
--   /generic/target_update/tr_target-update_table-updatable_table
--
--    AND (
--            pedd.last_updated_on IS NULL OR                          -- Rows that have been inserted but not modified.
--            pedd.last_updated_on &lt;= '${PARAM_CDC_CURRENT_LOAD_UTC}'  -- Do not load rows that are *modified* after the job starts.
--    )                                                                -- These restrictions are meant to avoid setting a foreign 
--                                                                     -- key to a row that is not yet inserted into the target DSA.
--                                                                     -- We do not need to specify similar conditions for the 
--                                                                     -- "peddd" table.
ORDER BY
    pedd.pedd_id    -- Insert new rows in the order they are created in the source PSA DB</sql>
    <limit>0</limit>
    <lookup>Write to log: parameters, max id &amp; timestamps</lookup>
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>448</xloc>
      <yloc>352</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Table output - DSA.schema.table</name>
    <type>TableOutput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dsa_db</connection>
    <schema>${PARAM_TARGET_SCHEMA}</schema>
    <table>${PARAM_TARGET_TABLE}</table>
    <commit>1000</commit>
    <truncate>N</truncate>
    <ignore_errors>N</ignore_errors>
    <use_batch>Y</use_batch>
    <specify_fields>N</specify_fields>
    <partitioning_enabled>N</partitioning_enabled>
    <partitioning_field />
    <partitioning_daily>N</partitioning_daily>
    <partitioning_monthly>Y</partitioning_monthly>
    <tablename_in_field>N</tablename_in_field>
    <tablename_field />
    <tablename_in_table>Y</tablename_in_table>
    <return_keys>N</return_keys>
    <return_field />
    <fields>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>336</xloc>
      <yloc>800</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Table output - DSA.schema.table 2</name>
    <type>TableOutput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dsa_db</connection>
    <schema>${PARAM_TARGET_SCHEMA}</schema>
    <table>${PARAM_TARGET_TABLE}</table>
    <commit>100</commit>
    <truncate>N</truncate>
    <ignore_errors>N</ignore_errors>
    <use_batch>N</use_batch>
    <specify_fields>N</specify_fields>
    <partitioning_enabled>N</partitioning_enabled>
    <partitioning_field />
    <partitioning_daily>N</partitioning_daily>
    <partitioning_monthly>Y</partitioning_monthly>
    <tablename_in_field>N</tablename_in_field>
    <tablename_field />
    <tablename_in_table>Y</tablename_in_table>
    <return_keys>N</return_keys>
    <return_field />
    <fields>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>1056</xloc>
      <yloc>496</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Update - DSA.schema.table</name>
    <type>Update</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dsa_db</connection>
    <skip_lookup>Y</skip_lookup>
    <commit>1000</commit>
    <use_batch>Y</use_batch>
    <error_ignored>N</error_ignored>
    <ignore_flag_field />
    <lookup>
      <schema>${PARAM_TARGET_SCHEMA}</schema>
      <table>${PARAM_TARGET_TABLE}</table>
      <key>
        <name>pedd_id</name>
        <field>pedd_id</field>
        <condition>=</condition>
        <name2 />
      </key>
      <value>
        <name>pedd_status_id</name>
        <rename>pedd_status_id</rename>
      </value>
      <value>
        <name>applied_axle_tariff_category_id</name>
        <rename>applied_axle_tariff_category_id</rename>
      </value>
      <value>
        <name>applied_euro_emission_class_id</name>
        <rename>applied_euro_emission_class_id</rename>
      </value>
      <value>
        <name>applied_lpn_country_code</name>
        <rename>applied_lpn_country_code</rename>
      </value>
      <value>
        <name>base_rate_total</name>
        <rename>base_rate_total</rename>
      </value>
      <value>
        <name>applied_infrastructure_fee</name>
        <rename>applied_infrastructure_fee</rename>
      </value>
      <value>
        <name>applied_infrastructure_vat</name>
        <rename>applied_infrastructure_vat</rename>
      </value>
      <value>
        <name>applied_surcharge</name>
        <rename>applied_surcharge</rename>
      </value>
      <value>
        <name>applied_surcharge_vat</name>
        <rename>applied_surcharge_vat</rename>
      </value>
      <value>
        <name>applied_external_fee_noise</name>
        <rename>applied_external_fee_noise</rename>
      </value>
      <value>
        <name>applied_external_fee_noise_vat</name>
        <rename>applied_external_fee_noise_vat</rename>
      </value>
      <value>
        <name>applied_external_fee_air</name>
        <rename>applied_external_fee_air</rename>
      </value>
      <value>
        <name>applied_external_fee_air_vat</name>
        <rename>applied_external_fee_air_vat</rename>
      </value>
      <value>
        <name>created_on</name>
        <rename>created_on</rename>
      </value>
      <value>
        <name>last_updated_on</name>
        <rename>last_updated_on</rename>
      </value>
      <value>
        <name>peddd_created_on</name>
        <rename>peddd_created_on</rename>
      </value>
      <value>
        <name>peddd_last_updated_on</name>
        <rename>peddd_last_updated_on</rename>
      </value>
      <value>
        <name>etl_batch_id_last_update</name>
        <rename>etl_batch_id_insert</rename>
      </value>
    </lookup>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>560</xloc>
      <yloc>800</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Update - DSA.schema.table 2</name>
    <type>Update</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>dsa_db</connection>
    <skip_lookup>N</skip_lookup>
    <commit>100</commit>
    <use_batch>Y</use_batch>
    <error_ignored>N</error_ignored>
    <ignore_flag_field />
    <lookup>
      <schema>${PARAM_TARGET_SCHEMA}</schema>
      <table>${PARAM_TARGET_TABLE}</table>
      <key>
        <name>pedd_id</name>
        <field>pedd_id</field>
        <condition>=</condition>
        <name2 />
      </key>
      <value>
        <name>pedd_status_id</name>
        <rename>pedd_status_id</rename>
      </value>
      <value>
        <name>applied_axle_tariff_category_id</name>
        <rename>applied_axle_tariff_category_id</rename>
      </value>
      <value>
        <name>applied_euro_emission_class_id</name>
        <rename>applied_euro_emission_class_id</rename>
      </value>
      <value>
        <name>applied_lpn_country_code</name>
        <rename>applied_lpn_country_code</rename>
      </value>
      <value>
        <name>base_rate_total</name>
        <rename>base_rate_total</rename>
      </value>
      <value>
        <name>applied_infrastructure_fee</name>
        <rename>applied_infrastructure_fee</rename>
      </value>
      <value>
        <name>applied_infrastructure_vat</name>
        <rename>applied_infrastructure_vat</rename>
      </value>
      <value>
        <name>applied_surcharge</name>
        <rename>applied_surcharge</rename>
      </value>
      <value>
        <name>applied_surcharge_vat</name>
        <rename>applied_surcharge_vat</rename>
      </value>
      <value>
        <name>applied_external_fee_noise</name>
        <rename>applied_external_fee_noise</rename>
      </value>
      <value>
        <name>applied_external_fee_noise_vat</name>
        <rename>applied_external_fee_noise_vat</rename>
      </value>
      <value>
        <name>applied_external_fee_air</name>
        <rename>applied_external_fee_air</rename>
      </value>
      <value>
        <name>applied_external_fee_air_vat</name>
        <rename>applied_external_fee_air_vat</rename>
      </value>
      <value>
        <name>created_on</name>
        <rename>created_on</rename>
      </value>
      <value>
        <name>last_updated_on</name>
        <rename>last_updated_on</rename>
      </value>
      <value>
        <name>peddd_created_on</name>
        <rename>peddd_created_on</rename>
      </value>
      <value>
        <name>peddd_last_updated_on</name>
        <rename>peddd_last_updated_on</rename>
      </value>
      <value>
        <name>etl_batch_id_last_update</name>
        <rename>etl_batch_id_insert</rename>
      </value>
    </lookup>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>1056</xloc>
      <yloc>592</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Write to log: parameters, max id &amp; timestamps</name>
    <type>WriteToLog</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <loglevel>log_level_basic</loglevel>
    <displayHeader>Y</displayHeader>
    <limitRows>N</limitRows>
    <limitRowsNumber>0</limitRowsNumber>
    <logmessage>tr_dsa-update_table-public.obo__passage_event_derived_data_with_rating:
PARAM_SOURCE_TABLE_MAX_INSERT_ID = ${PARAM_SOURCE_TABLE_MAX_INSERT_ID}</logmessage>
    <fields>
      <field>
        <name>max_target_insert_id_1</name>
      </field>
      <field>
        <name>max_target_insert_id_2</name>
      </field>
      <field>
        <name>max_target_last_updated_on</name>
      </field>
      <field>
        <name>max_target_peddd_created_on</name>
      </field>
      <field>
        <name>max_target_peddd_last_updated_on</name>
      </field>
    </fields>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>448</xloc>
      <yloc>272</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step_error_handling>
    <error>
      <source_step>Insert / Update- DSA.schema.table</source_step>
      <target_step />
      <is_enabled>N</is_enabled>
      <nr_valuename />
      <descriptions_valuename />
      <fields_valuename />
      <codes_valuename />
      <max_errors />
      <max_pct_errors />
      <min_pct_rows />
    </error>
    <error>
      <source_step>Table output - DSA.schema.table 2</source_step>
      <target_step>Update - DSA.schema.table 2</target_step>
      <is_enabled>N</is_enabled>
      <nr_valuename />
      <descriptions_valuename />
      <fields_valuename />
      <codes_valuename />
      <max_errors />
      <max_pct_errors />
      <min_pct_rows />
    </error>
  </step_error_handling>
  <slave-step-copy-partition-distribution>
  </slave-step-copy-partition-distribution>
  <slave_transformation>N</slave_transformation>
</transformation>
