<transformation>
  <info>
    <name>tr_psa_delete_archived_rows-inject_meta</name>
    <description />
    <extended_description />
    <trans_version />
    <trans_type>Normal</trans_type>
    <trans_status>0</trans_status>
    <directory>/psa</directory>
    <parameters>
      <parameter>
        <name>PARAM_COMPARE_BEFORE_DELETE</name>
        <default_value>N</default_value>
        <description>"Y" if rows to be deleted shall first be compared to rows in target PSA table</description>
      </parameter>
      <parameter>
        <name>PARAM_LAST_LOAD_TIMESTAMP</name>
        <default_value>1900.01.01 00:00:00</default_value>
        <description>Timestamp of last successful load from the source database</description>
      </parameter>
      <parameter>
        <name>PARAM_LAST_UPDATED_ON_SOURCE_COLNAME</name>
        <default_value />
        <description>Column of source table that acts as a "last_updated_on" timestamp column</description>
      </parameter>
      <parameter>
        <name>PARAM_ROW_CAN_BE_DELETED_FROM_SECONDS</name>
        <default_value>2000000000</default_value>
        <description>Number of seconds after the "row_can_be_deleted_from" timestamp that an archived row can be deleted</description>
      </parameter>
      <parameter>
        <name>PARAM_ROW_CAN_BE_DELETED_FROM_SOURCE_COLNAME</name>
        <default_value />
        <description>Column of source table that acts as a "row_can_be_deleted_from" timestamp column</description>
      </parameter>
      <parameter>
        <name>PARAM_SOURCE_DB_ID</name>
        <default_value>1</default_value>
        <description>ID the "source" database containing tables will be mirrored to a "target" database</description>
      </parameter>
      <parameter>
        <name>PARAM_SOURCE_SCHEMA</name>
        <default_value>passage</default_value>
        <description>Name of schema containing source table to mirror to target table</description>
      </parameter>
      <parameter>
        <name>PARAM_SOURCE_TABLE</name>
        <default_value>passage</default_value>
        <description>Name of source table to mirror to target table</description>
      </parameter>
      <parameter>
        <name>PARAM_TABLE_META_ID</name>
        <default_value>615</default_value>
        <description>Primary key of etl.table_meta row for "source" table from which rows will be deleted</description>
      </parameter>
      <parameter>
        <name>PARAM_TARGET_SCHEMA</name>
        <default_value>passage</default_value>
        <description>Name of schema containing target table to update (not currently used)</description>
      </parameter>
      <parameter>
        <name>PARAM_TARGET_TABLE</name>
        <default_value>passage</default_value>
        <description>Name of target table to update (not currently used)</description>
      </parameter>
      <parameter>
        <name>PARAM_USE_CUSTOM_DELETE_ALGORITHM</name>
        <default_value>N</default_value>
        <description>"Y" if a custom DELETE algorithm shall be used for this table</description>
      </parameter>
    </parameters>
    <log>
      <trans-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANS_SCHEMA}</schema>
        <table>${QF_LOG_TRANS_TABLE}</table>
        <size_limit_lines />
        <interval />
        <timeout_days>${QF_LOG_TRANS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STATUS</id>
          <enabled>Y</enabled>
          <name>STATUS</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
          <subject />
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
          <subject />
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
          <subject />
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
          <subject />
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
          <subject />
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
          <subject />
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>STARTDATE</id>
          <enabled>Y</enabled>
          <name>STARTDATE</name>
        </field>
        <field>
          <id>ENDDATE</id>
          <enabled>Y</enabled>
          <name>ENDDATE</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>DEPDATE</id>
          <enabled>Y</enabled>
          <name>DEPDATE</name>
        </field>
        <field>
          <id>REPLAYDATE</id>
          <enabled>Y</enabled>
          <name>REPLAYDATE</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>Y</enabled>
          <name>LOG_FIELD</name>
        </field>
        <field>
          <id>EXECUTING_SERVER</id>
          <enabled>N</enabled>
          <name>EXECUTING_SERVER</name>
        </field>
        <field>
          <id>EXECUTING_USER</id>
          <enabled>N</enabled>
          <name>EXECUTING_USER</name>
        </field>
        <field>
          <id>CLIENT</id>
          <enabled>N</enabled>
          <name>CLIENT</name>
        </field>
      </trans-log-table>
      <perf-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANSPERFORMANCE_SCHEMA}</schema>
        <table>${QF_LOG_TRANSPERFORMANCE_TABLE}</table>
        <interval />
        <timeout_days>${QF_LOG_TRANSPERFORMANCE_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>SEQ_NR</id>
          <enabled>Y</enabled>
          <name>SEQ_NR</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>INPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>INPUT_BUFFER_ROWS</name>
        </field>
        <field>
          <id>OUTPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>OUTPUT_BUFFER_ROWS</name>
        </field>
      </perf-log-table>
      <channel-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_CHANNEL_SCHEMA}</schema>
        <table>${QF_LOG_CHANNEL_TABLE}</table>
        <timeout_days>${QF_LOG_CHANNEL_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>LOGGING_OBJECT_TYPE</id>
          <enabled>Y</enabled>
          <name>LOGGING_OBJECT_TYPE</name>
        </field>
        <field>
          <id>OBJECT_NAME</id>
          <enabled>Y</enabled>
          <name>OBJECT_NAME</name>
        </field>
        <field>
          <id>OBJECT_COPY</id>
          <enabled>Y</enabled>
          <name>OBJECT_COPY</name>
        </field>
        <field>
          <id>REPOSITORY_DIRECTORY</id>
          <enabled>Y</enabled>
          <name>REPOSITORY_DIRECTORY</name>
        </field>
        <field>
          <id>FILENAME</id>
          <enabled>Y</enabled>
          <name>FILENAME</name>
        </field>
        <field>
          <id>OBJECT_ID</id>
          <enabled>Y</enabled>
          <name>OBJECT_ID</name>
        </field>
        <field>
          <id>OBJECT_REVISION</id>
          <enabled>Y</enabled>
          <name>OBJECT_REVISION</name>
        </field>
        <field>
          <id>PARENT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>PARENT_CHANNEL_ID</name>
        </field>
        <field>
          <id>ROOT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>ROOT_CHANNEL_ID</name>
        </field>
      </channel-log-table>
      <step-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANSSTEP_SCHEMA}</schema>
        <table>${QF_LOG_TRANSSTEP_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSSTEP_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>N</enabled>
          <name>LOG_FIELD</name>
        </field>
      </step-log-table>
      <metrics-log-table>
        <connection>logging_db</connection>
        <schema>${QF_LOG_TRANSMETRICS_SCHEMA}</schema>
        <table>${QF_LOG_TRANSMETRICS_TABLE}</table>
        <timeout_days>${QF_LOG_TRANSMETRICS_TIMEOUT_IN_DAYS}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>METRICS_DATE</id>
          <enabled>Y</enabled>
          <name>METRICS_DATE</name>
        </field>
        <field>
          <id>METRICS_CODE</id>
          <enabled>Y</enabled>
          <name>METRICS_CODE</name>
        </field>
        <field>
          <id>METRICS_DESCRIPTION</id>
          <enabled>Y</enabled>
          <name>METRICS_DESCRIPTION</name>
        </field>
        <field>
          <id>METRICS_SUBJECT</id>
          <enabled>Y</enabled>
          <name>METRICS_SUBJECT</name>
        </field>
        <field>
          <id>METRICS_TYPE</id>
          <enabled>Y</enabled>
          <name>METRICS_TYPE</name>
        </field>
        <field>
          <id>METRICS_VALUE</id>
          <enabled>Y</enabled>
          <name>METRICS_VALUE</name>
        </field>
      </metrics-log-table>
    </log>
    <maxdate>
      <connection />
      <table />
      <field />
      <offset>0.0</offset>
      <maxdiff>0.0</maxdiff>
    </maxdate>
    <size_rowset>10000</size_rowset>
    <sleep_time_empty>50</sleep_time_empty>
    <sleep_time_full>50</sleep_time_full>
    <unique_connections>N</unique_connections>
    <feedback_shown>Y</feedback_shown>
    <feedback_size>50000</feedback_size>
    <using_thread_priorities>Y</using_thread_priorities>
    <shared_objects_file />
    <capture_step_performance>Y</capture_step_performance>
    <step_performance_capturing_delay>1000</step_performance_capturing_delay>
    <step_performance_capturing_size_limit>100</step_performance_capturing_size_limit>
    <dependencies>
    </dependencies>
    <partitionschemas>
    </partitionschemas>
    <slaveservers>
    </slaveservers>
    <clusterschemas>
    </clusterschemas>
    <created_user>-</created_user>
    <created_date>2016/04/03 09:57:16.279</created_date>
    <modified_user>-</modified_user>
    <modified_date>2017/07/19 09:39:12.298</modified_date>
    <key_for_session_key>H4sIAAAAAAAAAAMAAAAAAAAAAAA=</key_for_session_key>
    <is_key_private>N</is_key_private>
  </info>
  <notepads>
    <notepad>
      <note>This transformation uses ETL Metadata injection to delete rows of a source table in a source database that have been archived/mirrored to a target table in a
target database. This is a generic algorithm that can work with *any* table, as long as the necessary metadata is available to be injected. This transformation
has parameters:

	PARAM_SOURCE_DB_ID								ID the "source" database containing tables will be mirrored to a "target" database
	PARAM_LAST_LOAD_TIMESTAMP					Timestamp for when the target PSA DB mirror tables were last updated from the source DB
	PARAM_SOURCE_SCHEMA							Name of schema containing source table to compare to target table
	PARAM_SOURCE_TABLE							Name of source table to compare to target table
	PARAM_TARGET_SCHEMA							Name of schema containing target table to compare with (not currently used)
	PARAM_TARGET_TABLE								Name of target table to compare with (not currently used)
	PARAM_COMPARE_BEFORE_DELETE					"Y" if rows to be deleted shall first be compared to rows in target PSA table
	PARAM_ROW_CAN_BE_DELETED_FROM_SECONDS	Number of seconds after the "row_can_be_deleted_from" timestamp that an archived row can be deleted
	PARAM_USE_CUSTOM_DELETE_ALGORITHM			"Y" if a custom DELETE algorithm shall be used for this table
	PARAM_ROW_CAN_BE_DELETED_FROM_SOURCE_COLNAME	Column of source table that acts as a "row_can_be_deleted_from" timestamp column
	PARAM_LAST_UPDATED_ON_SOURCE_COLNAME	Column of source table that acts as a "last_updated_on" timestamp column
	PARAM_TABLE_META_ID							Primary key of etl.table_meta row for "source" table from which rows will be deleted</note>
      <xloc>0</xloc>
      <yloc>0</yloc>
      <width>963</width>
      <heigth>234</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>The query here to provide the source (OBO)
column names must match the query in 
"jb_psa_delete_archived_rows-table" from
where this transformation is executed. See
the comments in that job for an explanation.</note>
      <xloc>304</xloc>
      <yloc>528</yloc>
      <width>269</width>
      <heigth>80</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>10</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>255</backgroundcolorgreen>
      <backgroundcolorblue>0</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <connection>
    <name>etl_db</name>
    <server>${QF_ETL_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_ETL_DB_DATABASE}</database>
    <port>${QF_ETL_DB_PORT}</port>
    <username>${QF_ETL_DB_USERNAME}</username>
    <password>${QF_ETL_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_ETL_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <connection>
    <name>logging_db</name>
    <server>${QF_LOGGING_DB_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${QF_LOGGING_DB_DATABASE}</database>
    <port>${QF_LOGGING_DB_PORT}</port>
    <username>${QF_LOGGING_DB_USERNAME}</username>
    <password>${QF_LOGGING_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${QF_LOGGING_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <order>
    <hop>
      <from>SELECT statement for target (PSA) DB</from>
      <to>ETL Metadata Injection: tmplt_psa_delete_archived_rows</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Target (PSA) column names to compare</from>
      <to>ETL Metadata Injection: tmplt_psa_delete_archived_rows</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Source (OBO) primary key column names</from>
      <to>ETL Metadata Injection: tmplt_psa_delete_archived_rows</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Source (OBO) column names to compare</from>
      <to>ETL Metadata Injection: tmplt_psa_delete_archived_rows</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>SELECT statement for source (OBO) DB</from>
      <to>ETL Metadata Injection: tmplt_psa_delete_archived_rows</to>
      <enabled>Y</enabled>
    </hop>
  </order>
  <step>
    <name>ETL Metadata Injection: tmplt_psa_delete_archived_rows</name>
    <type>MetaInject</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <specification_method>rep_name</specification_method>
    <trans_object_id />
    <trans_name>tmplt_psa_delete_archived_rows</trans_name>
    <filename />
    <directory_path>${Internal.Entry.Current.Directory}</directory_path>
    <source_step />
    <source_output_fields>    </source_output_fields>
    <target_file>/tmp/tr_psa_delete_archived_rows.ktr</target_file>
    <no_execution>Y</no_execution>
    <stream_source_step />
    <stream_target_step />
    <mappings>
      <mapping>
        <target_step_name>Get rows from source (OBO) DB table</target_step_name>
        <target_attribute_key>SQL</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step>SELECT statement for source (OBO) DB</source_step>
        <source_field>select_source_rows_sql</source_field>
      </mapping>
      <mapping>
        <target_step_name>Rename PSA field names -> OBO field names</target_step_name>
        <target_attribute_key>FIELD_RENAME</target_attribute_key>
        <target_detail>Y</target_detail>
        <source_step>Source (OBO) column names to compare</source_step>
        <source_field>source_col_name</source_field>
      </mapping>
      <mapping>
        <target_step_name>Merge Rows (diff)</target_step_name>
        <target_attribute_key>KEY_FIELDS</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step>Source (OBO) primary key column names</source_step>
        <source_field>key_col_name</source_field>
      </mapping>
      <mapping>
        <target_step_name>Merge Rows (diff)</target_step_name>
        <target_attribute_key>VALUE_FIELDS</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step>Source (OBO) column names to compare</source_step>
        <source_field>source_col_name</source_field>
      </mapping>
      <mapping>
        <target_step_name>Get rows from target (PSA) DB table</target_step_name>
        <target_attribute_key>SQL</target_attribute_key>
        <target_detail>N</target_detail>
        <source_step>SELECT statement for target (PSA) DB</source_step>
        <source_field>select_target_rows_sql</source_field>
      </mapping>
      <mapping>
        <target_step_name>Rename PSA field names -> OBO field names</target_step_name>
        <target_attribute_key>FIELD_NAME</target_attribute_key>
        <target_detail>Y</target_detail>
        <source_step>Target (PSA) column names to compare</source_step>
        <source_field>target_col_name</source_field>
      </mapping>
    </mappings>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>416</xloc>
      <yloc>352</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>SELECT statement for source (OBO) DB</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>etl_db</connection>
    <sql>WITH
    -- Create table with a string column that contains the columns from the 
    -- source (OBO) table to compare with corresponding columns from the 
    -- target (PSA) table:
    select_list AS (
        SELECT 
            -- POSTGRESQL-SPECIFIC
            array_to_string(array_agg(source_column_name ORDER BY column_meta_id ASC), ',') AS "str_value"
        FROM 
            etl.column_meta 
        WHERE 
            table_meta_id = ${PARAM_TABLE_META_ID} AND (
                -- Although the primary key columns don't need to be compared, they
                -- are needed by the "Merge Rows" step so they are included here.
                -- This also ensures that there will be columns to compare.
                is_primary_key_column=true OR              -- always compare this/these column(s)
                is_last_updated_on_column=true OR          -- always compare this column, if defined
                is_row_can_be_deleted_from_column=true OR  -- always compare this column, if defined
                (compare_column=true AND '${PARAM_COMPARE_BEFORE_DELETE}'='Y')
            )
    ),
    -- Create table with a string column that contains the "ORDER BY" clause. 
    -- This consists of the columns that make up the primary key for the 
    -- source (OBO) table. This is needed by the "Merge Rows" step in the 
    -- template transformation.
    orderby_list AS (
        SELECT 
           -- POSTGRESQL-SPECIFIC
            array_to_string(array_agg(source_column_name ORDER BY primary_key_column_order ASC), ',') AS "str_value"
        FROM 
            etl.column_meta 
        WHERE 
            table_meta_id = ${PARAM_TABLE_META_ID} AND 
            is_primary_key_column = true
    )
SELECT
    -- Create SELECT statement to select rows from source (OBO) table to compare/delete.
    -- The WHERE clause includes only rows that should have been archived/mirrored.
    'SELECT ' || CAST(select_list.str_value AS VARCHAR) || 
    ' FROM ${PARAM_SOURCE_SCHEMA}.${PARAM_SOURCE_TABLE} ' || 
    'WHERE ${PARAM_ROW_CAN_BE_DELETED_FROM_SOURCE_COLNAME} + INTERVAL ''${PARAM_ROW_CAN_BE_DELETED_FROM_SECONDS} seconds'' &lt; TIMESTAMP ''${PARAM_LAST_LOAD_TIMESTAMP}'' ' ||
    'ORDER BY ' || CAST(orderby_list.str_value AS VARCHAR) 
    AS "select_source_rows_sql" 
FROM
    select_list
CROSS JOIN
    orderby_list</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>272</xloc>
      <yloc>256</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>SELECT statement for target (PSA) DB</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>etl_db</connection>
    <sql>WITH
    -- Create table with a string column that contains the columns from the 
    -- target (PSA) table to compare with corresponding columns from the 
    -- source (OBO) table:
    select_list AS (
        SELECT 
           -- POSTGRESQL-SPECIFIC
            array_to_string(array_agg(target_column_name ORDER BY column_meta_id ASC), ',') AS "str_value"
        FROM 
            etl.column_meta 
        WHERE 
            table_meta_id = ${PARAM_TABLE_META_ID} AND (
                -- Although the primary key columns don't need to be compared, they
                -- are needed by the "Merge Rows" step so they are included here.
                -- This also ensures that there will be columns to compare.
                is_primary_key_column=true OR              -- always compare this/these column(s)
                is_last_updated_on_column=true OR          -- always compare this column, if defined
                is_row_can_be_deleted_from_column=true OR  -- always compare this column, if defined
                (compare_column=true AND '${PARAM_COMPARE_BEFORE_DELETE}'='Y')
            )
    ),
    -- Create table with a string column that contains the "ORDER BY" clause. 
    -- This consists of the columns that make up the primary key for the 
    -- target (PSA) table. This is needed by the "Merge Rows" step in the 
    -- template transformation.
    orderby_list AS (
        SELECT 
           -- POSTGRESQL-SPECIFIC
            array_to_string(array_agg(target_column_name ORDER BY primary_key_column_order ASC), ',') AS "str_value"
        FROM 
            etl.column_meta 
        WHERE 
            table_meta_id = ${PARAM_TABLE_META_ID} AND 
            is_primary_key_column = true
    )
SELECT
    -- Create SELECT statement to select rows from target (PSA) table to compare.
    -- The WHERE clause includes only rows that should have been archived/mirrored.
    'SELECT ' || CAST(select_list.str_value AS VARCHAR) || 
    ' FROM ${PARAM_TARGET_SCHEMA}.${PARAM_TARGET_TABLE} ' || 
    'WHERE ${PARAM_ROW_CAN_BE_DELETED_FROM_SOURCE_COLNAME} + INTERVAL ''${PARAM_ROW_CAN_BE_DELETED_FROM_SECONDS} seconds'' &lt; TIMESTAMP ''${PARAM_LAST_LOAD_TIMESTAMP}'' ' ||
    'ORDER BY ' || CAST(orderby_list.str_value AS VARCHAR) 
    AS "select_target_rows_sql" 
FROM
    select_list
CROSS JOIN
    orderby_list</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>608</xloc>
      <yloc>256</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Source (OBO) column names to compare</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>etl_db</connection>
    <sql>SELECT
    source_column_name AS "source_col_name" 
FROM 
    etl.column_meta 
WHERE 
    table_meta_id = ${PARAM_TABLE_META_ID} AND (
        -- Although the primary key columns don't need to be compared, they
        -- are needed by the "Merge Rows" step so they are included here.
        -- This also ensures that there will be columns to compare.
        is_primary_key_column=true OR              -- always compare this/these column(s)
        is_last_updated_on_column=true OR          -- always compare this column, if defined
        is_row_can_be_deleted_from_column=true OR  -- always compare this column, if defined
        (compare_column=true AND '${PARAM_COMPARE_BEFORE_DELETE}'='Y')
    )
ORDER BY 
    column_meta_id ASC  -- must use the same sort order in "Target (PSA) column names to compare"</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>112</xloc>
      <yloc>352</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Source (OBO) primary key column names</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>etl_db</connection>
    <sql>SELECT
    source_column_name AS "key_col_name" 
FROM 
    etl.column_meta 
WHERE 
    table_meta_id = ${PARAM_TABLE_META_ID} AND 
    is_primary_key_column = true
ORDER BY 
    primary_key_column_order</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>416</xloc>
      <yloc>464</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>Target (PSA) column names to compare</name>
    <type>TableInput</type>
    <description />
    <distribute>Y</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>etl_db</connection>
    <sql>SELECT
    target_column_name AS "target_col_name" 
FROM 
    etl.column_meta 
WHERE 
    table_meta_id = ${PARAM_TABLE_META_ID} AND (
        -- Although the primary key columns don't need to be compared, they
        -- are needed by the "Merge Rows" step so they are included here.
        -- This also ensures that there will be columns to compare.
        is_primary_key_column=true OR              -- always compare this/these column(s)
        is_last_updated_on_column=true OR          -- always compare this column, if defined
        is_row_can_be_deleted_from_column=true OR  -- always compare this column, if defined
        (compare_column=true AND '${PARAM_COMPARE_BEFORE_DELETE}'='Y')
    )
ORDER BY 
    column_meta_id ASC  -- must use the same sort order in "Source (OBO) column names to compare"</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>720</xloc>
      <yloc>352</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step_error_handling>
  </step_error_handling>
  <slave-step-copy-partition-distribution>
  </slave-step-copy-partition-distribution>
  <slave_transformation>N</slave_transformation>
</transformation>
